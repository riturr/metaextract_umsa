{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-17T01:36:48.670883739Z",
     "start_time": "2023-11-17T01:36:47.997908398Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Carga de los datos\n",
    "documents_to_process_reduced = pd.read_pickle('documents_to_process_reduced.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6a517caf66ba7f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Construcción del conjunto de datos de entrenamiento"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4983c01fb1176971"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_lg\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T23:25:06.478587657Z",
     "start_time": "2023-11-23T23:25:05.251860367Z"
    }
   },
   "id": "4784d052c808906d"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from typing import Any\n",
    "from spacy.tokens import Span, Doc\n",
    "from spacy.matcher import Matcher\n",
    "from spacy import displacy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "Doc.set_extension('pdf_path', default=None, force=True)\n",
    "Doc.set_extension('xml_path', default=None, force=True)\n",
    "\n",
    "def build_patterns(text: str) ->  list[list[dict[str, Any]]]:\n",
    "    doc = nlp(text)\n",
    "    if len(doc) == 0:\n",
    "        return []\n",
    "    reversed_doc = list(doc)\n",
    "    reversed_doc.reverse()\n",
    "    return [\n",
    "        reduce(lambda rules_list, span: rules_list + [{'LOWER': span.text.lower()}, {'IS_SPACE': True, 'OP': '?'}], doc, [])[:-1],\n",
    "        reduce(lambda rules_list, span: rules_list + [{'LOWER': span.text.lower()}, {'IS_SPACE': True, 'OP': '?'}], reversed_doc, [])[:-1]\n",
    "    ]\n",
    "\n",
    "def build_matcher(entities: list[dict]):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    for entity in entities:\n",
    "        if entity['TEXT'] is None:\n",
    "            continue\n",
    "        if type(entity['TEXT']) == float and np.isnan(entity['TEXT']):\n",
    "            continue\n",
    "        patterns = build_patterns(entity['TEXT'])\n",
    "        matcher.add(entity['ENTITY_TYPE'], patterns)\n",
    "    return matcher\n",
    "\n",
    "def is_overlapping(match1, match2):\n",
    "    start_1 = match1[1]\n",
    "    end_1 = match1[2]\n",
    "    start_2 = match2[1]\n",
    "    end_2 = match2[2]\n",
    "    return end_1 >= start_2 and end_2 >= start_1\n",
    "    \n",
    "def resolve_matches_conflicts(matches: list[tuple[int,int, int]]):\n",
    "    result = []\n",
    "    priority_order = ['TITLE', 'AUTHOR', 'ADVISOR', 'YEAR', 'FACULTY', 'DEPARTMENT']\n",
    "    for p in priority_order:\n",
    "        filtered_matches = filter(lambda match: nlp.vocab.strings[match[0]] == p, matches)\n",
    "        for m in filtered_matches:\n",
    "            overlaps_with_existing_match = any([is_overlapping(existing_match, m) for existing_match in result])\n",
    "            if not overlaps_with_existing_match:\n",
    "                result.append(m)\n",
    "    return result\n",
    "            \n",
    "def get_training_data(row, entities: list[dict]):\n",
    "    try:\n",
    "        matcher = build_matcher(entities)\n",
    "        doc = nlp(row['pdf_cover_page'])\n",
    "        doc._.pdf_path = row['pdf_path']\n",
    "        doc._.xml_path = row['xml_path']\n",
    "        matches = matcher(doc)\n",
    "        matches = resolve_matches_conflicts(matches)\n",
    "        entities = [Span(doc, start=match[1], end=match[2], label=nlp.vocab.strings[match[0]]) for match in matches]\n",
    "        #### for ent in entities:\n",
    "        ####    print(ent.text, ent.start_char, ent.end_char)\n",
    "        doc.set_ents(entities=entities)\n",
    "        return doc\n",
    "    except ValueError as e:\n",
    "        if '[E1010]' in str(e):\n",
    "            print(\"WARNING\", e)\n",
    "            return np.nan\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "def get_training_data_for_all_entities(row):\n",
    "    entities = [\n",
    "        {'ENTITY_TYPE': 'TITLE', 'TEXT': row['title']},\n",
    "        {'ENTITY_TYPE': 'FACULTY', 'TEXT': row['faculty']},\n",
    "        {'ENTITY_TYPE': 'DEPARTMENT', 'TEXT': row['department']},\n",
    "        {'ENTITY_TYPE': 'YEAR', 'TEXT': str(row['year'])}, ## year overlaps with title as many titles has years in it\n",
    "        *[{'ENTITY_TYPE': 'AUTHOR', 'TEXT': f'{author[\"given_names\"]} {author[\"last_names\"]}'} for author in row['authors']],\n",
    "        *[{'ENTITY_TYPE': 'ADVISOR', 'TEXT': f'{author[\"given_names\"]} {author[\"last_names\"]}'} for author in row['advisors']],\n",
    "    ]\n",
    "    return get_training_data(row, entities)\n",
    "\n",
    "def get_training_data_for_advisors(row):\n",
    "    entities = [\n",
    "        *[{'ENTITY_TYPE': 'ADVISOR', 'TEXT': f'{author[\"given_names\"]} {author[\"last_names\"]}'} for author in row['advisors']],\n",
    "    ]\n",
    "    return get_training_data(row, entities)\n",
    "\n",
    "def get_training_data_for_authors(row):\n",
    "    entities = [\n",
    "        *[{'ENTITY_TYPE': 'AUTHOR', 'TEXT': f'{author[\"given_names\"]} {author[\"last_names\"]}'} for author in row['authors']],\n",
    "    ]\n",
    "    return get_training_data(row, entities)\n",
    "\n",
    "def get_training_data_for_title(row):\n",
    "    entities = [\n",
    "        {'ENTITY_TYPE': 'TITLE', 'TEXT': row['title']},\n",
    "    ]\n",
    "    return get_training_data(row, entities)\n",
    "\n",
    "def get_training_data_for_faculty(row):\n",
    "    entities = [\n",
    "        {'ENTITY_TYPE': 'FACULTY', 'TEXT': row['faculty']},\n",
    "    ]\n",
    "    return get_training_data(row, entities)\n",
    "\n",
    "def get_training_data_for_department(row):\n",
    "    entities = [\n",
    "        {'ENTITY_TYPE': 'DEPARTMENT', 'TEXT': row['department']},\n",
    "    ]\n",
    "    return get_training_data(row, entities)\n",
    "\n",
    "def get_training_data_for_names(row):\n",
    "    entities = [\n",
    "        *[{'ENTITY_TYPE': 'LAST_NAME', 'TEXT': f'{author[\"last_names\"]}'} for author in row['authors']],\n",
    "        *[{'ENTITY_TYPE': 'GIVEN_NAME', 'TEXT': f'{author[\"given_names\"]}'} for author in row['authors']],\n",
    "    ]\n",
    "    \n",
    "    matcher = build_matcher(entities)\n",
    "    names = []\n",
    "    for author in row['authors']:\n",
    "        name = list(author.values())\n",
    "        random.shuffle(name)\n",
    "        names.append(' '.join(name))\n",
    "    names = '\\n'.join(names)\n",
    "    try:\n",
    "        doc = nlp(names)\n",
    "        doc._.pdf_path = row['pdf_path']\n",
    "        doc._.xml_path = row['xml_path']\n",
    "        matches= matcher(doc)\n",
    "        entities = [Span(doc, start=match[1], end=match[2], label=nlp.vocab.strings[match[0]]) for match in matches]\n",
    "        doc.set_ents(entities=entities)\n",
    "        return doc\n",
    "    except ValueError as e:\n",
    "        if '[E1010]' in str(e):\n",
    "            print(\"WARNING\", e)\n",
    "            return np.nan\n",
    "        else:\n",
    "            raise e\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T23:25:07.872723323Z",
     "start_time": "2023-11-23T23:25:07.855549429Z"
    }
   },
   "id": "186bd25d81ff4e49"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import DocBin\n",
    "from random import seed, shuffle\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "def generate_training_files(generator, output_train_file, ouput_test_file):\n",
    "    docs = documents_to_process_reduced.parallel_apply(generator, axis='columns')\n",
    "    ##### docs = documents_to_process_reduced[0:1].parallel_apply(generator, axis='columns')\n",
    "    \n",
    "    filtered_docs: pd.Series = docs.dropna()\n",
    "    \n",
    "    filtered_docs: pd.DataFrame = filtered_docs.apply(lambda doc: pd.Series([len(doc.ents), doc])).rename(columns={0: 'entities_count', 1: 'doc'})\n",
    "    filtered_docs = filtered_docs[filtered_docs['entities_count'] > 0]\n",
    "    \n",
    "    seed(42)\n",
    "    filtered_docs = list(filtered_docs['doc'])\n",
    "    shuffle(filtered_docs)\n",
    "    train_docs = filtered_docs[0:int(len(filtered_docs) // 1.25)]  # 80%\n",
    "    test_docs = filtered_docs[int(len(filtered_docs) // 1.25):]  # 20%\n",
    "    train_data = DocBin(docs=train_docs)\n",
    "    test_data = DocBin(docs=test_docs)\n",
    "    train_data.to_disk(output_train_file)\n",
    "    test_data.to_disk(ouput_test_file)\n",
    "    return docs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T23:25:31.860963273Z",
     "start_time": "2023-11-23T23:25:31.797589341Z"
    }
   },
   "id": "2a74ad0d98f3f761"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2681), Label(value='0 / 2681'))), …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86db4d6eef1543e1be0a4110a3a02ac9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# entity = 'all'\n",
    "generator_per_entity = {\n",
    "    'all': get_training_data_for_all_entities,\n",
    "    'title': get_training_data_for_title,\n",
    "    'authors': get_training_data_for_authors,\n",
    "    'advisors': get_training_data_for_advisors,\n",
    "    'faculty': get_training_data_for_faculty,\n",
    "    'department': get_training_data_for_department,\n",
    "    'names': get_training_data_for_names,\n",
    "}\n",
    "\n",
    "for entity in generator_per_entity:\n",
    "    print(f\"Generating training files for {entity}\")\n",
    "    training_docs = generate_training_files(generator_per_entity[entity], f'{entity}_train.spacy', f'{entity}_test.spacy')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T23:27:26.976427111Z",
     "start_time": "2023-11-23T23:25:36.579357975Z"
    }
   },
   "id": "2ef2b5e8f995298a"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "entity = \"all_year\"\n",
    "nlp_test = spacy.load(f'./{entity}_ner_model/model-best')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T23:44:22.522003811Z",
     "start_time": "2023-11-23T23:44:22.403883734Z"
    }
   },
   "id": "5c593a2c5076d7b"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">UNIVERSIDAD MAYOR DE SAN ANDRES<br>FACULTAD DE CIANCIAS FARMACEUTICAS Y BIOQUIMICAS<br>\n<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    CARRERA DE BIOQUIMICA\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DEPARTMENT</span>\n</mark>\n<br><br>FRECUENCIA DE LA TUBERCULOSIS PULMONAR A<br>TRAVES DE INFORMES DE BACILOSCOPIAS DE LA RED<br>DE LABORATORIOS REMITIDOS AL<br>LABORATORIO REGIONAL DE TUBERCULOSIS DE LA<br>CIUDAD DE EL ALTO, GESTION 2007-2008<br><br>AUTORA: \n<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    MARILYN DAYSI MORALES LOPEZ\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">AUTHOR</span>\n</mark>\n<br>ASESORES EXTERNOS: DR. CARLOS MARK TRUJILLO M.<br>DRA. RUTH MORALES DE TRUJILLO<br>ASESORES INTERNOS: DR. ENRIQUE TERRAZAS SILES<br>DR. LUIS FERNANDO SOSA<br><br>TRABAJO DIRIGIDO PARA OPTAR AL TITULO DE<br>LICENCIATURA EN BIOQUIMICA<br><br>LA PAZ - BOLIVIA<br><br>\n<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    2010\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">YEAR</span>\n</mark>\n<br></div></span>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPARTMENT: 'CARRERA DE BIOQUIMICA'\n",
      "AUTHOR: 'MARILYN DAYSI MORALES LOPEZ'\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "test_docs: pd.Series = training_docs.dropna()\n",
    "test_docs: pd.DataFrame = test_docs.apply(lambda doc: pd.Series([len(doc.ents), doc])).rename(columns={0: 'entities_count', 1: 'doc'})\n",
    "test_docs = test_docs[test_docs['entities_count'] < 3]\n",
    "test_doc = test_docs.iloc[random.randint(0, len(test_docs))]['doc']\n",
    "\n",
    "displacy.render(nlp_test(test_doc.text), style='ent')\n",
    "\n",
    "for ent in test_doc.ents:\n",
    "    print(f\"{ent.label_}: {repr(ent.text)}\")\n",
    "print(\"###\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T23:46:11.471413199Z",
     "start_time": "2023-11-23T23:46:10.543700547Z"
    }
   },
   "id": "905b68d0c8dfb404"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
