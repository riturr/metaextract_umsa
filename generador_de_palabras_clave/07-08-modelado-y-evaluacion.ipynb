{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65b80bda368827b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collected_data = pd.read_pickle(\"collected_data.pkl\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88257ce4e0a1ba72"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 42\n",
    "\n",
    "train_test_df: pd.DataFrame\n",
    "eval_df: pd.DataFrame\n",
    "\n",
    "train_test_df, eval_df = train_test_split(collected_data, test_size=0.1, random_state=seed, shuffle=True)\n",
    "print(f\"\"\"Tamaño del dataset de entrenamiento/pruebas: {len(train_test_df)}\n",
    "Tamaño del dataset de evaluación: {len(eval_df)}\"\"\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e7be447473906d6"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 26198.03it/s]\n",
      "/tmp/ipykernel_5060/1651845115.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tempDF['zero_shot_prompts'] = tempDF.progress_apply(lambda x: get_zero_shot_prompt(x['title'], x['abstract']), axis='columns')\n",
      "100%|██████████| 10/10 [00:00<00:00, 32922.32it/s]\n",
      "/tmp/ipykernel_5060/1651845115.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tempDF['one_shot_prompts'] = tempDF.progress_apply(lambda x: get_zero_shot_prompt(x['title'], x['abstract']), axis='columns')\n",
      "100%|██████████| 10/10 [00:00<00:00, 29392.46it/s]\n",
      "/tmp/ipykernel_5060/1651845115.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tempDF['few_shot_prompts'] = tempDF.progress_apply(lambda x: get_zero_shot_prompt(x['title'], x['abstract']), axis='columns')\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                    zero_shot_prompts  \\\n3   [INST] Extrae palabras clave desde el siguient...   \n4   [INST] Extrae palabras clave desde el siguient...   \n5   [INST] Extrae palabras clave desde el siguient...   \n7   [INST] Extrae palabras clave desde el siguient...   \n9   [INST] Extrae palabras clave desde el siguient...   \n13  [INST] Extrae palabras clave desde el siguient...   \n17  [INST] Extrae palabras clave desde el siguient...   \n18  [INST] Extrae palabras clave desde el siguient...   \n21  [INST] Extrae palabras clave desde el siguient...   \n23  [INST] Extrae palabras clave desde el siguient...   \n\n                                     one_shot_prompts  \\\n3   [INST] Extrae palabras clave desde el siguient...   \n4   [INST] Extrae palabras clave desde el siguient...   \n5   [INST] Extrae palabras clave desde el siguient...   \n7   [INST] Extrae palabras clave desde el siguient...   \n9   [INST] Extrae palabras clave desde el siguient...   \n13  [INST] Extrae palabras clave desde el siguient...   \n17  [INST] Extrae palabras clave desde el siguient...   \n18  [INST] Extrae palabras clave desde el siguient...   \n21  [INST] Extrae palabras clave desde el siguient...   \n23  [INST] Extrae palabras clave desde el siguient...   \n\n                                     few_shot_prompts  \n3   [INST] Extrae palabras clave desde el siguient...  \n4   [INST] Extrae palabras clave desde el siguient...  \n5   [INST] Extrae palabras clave desde el siguient...  \n7   [INST] Extrae palabras clave desde el siguient...  \n9   [INST] Extrae palabras clave desde el siguient...  \n13  [INST] Extrae palabras clave desde el siguient...  \n17  [INST] Extrae palabras clave desde el siguient...  \n18  [INST] Extrae palabras clave desde el siguient...  \n21  [INST] Extrae palabras clave desde el siguient...  \n23  [INST] Extrae palabras clave desde el siguient...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>zero_shot_prompts</th>\n      <th>one_shot_prompts</th>\n      <th>few_shot_prompts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n      <td>[INST] Extrae palabras clave desde el siguient...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only for reporting purposes\n",
    "def get_zero_shot_prompt(title, abstract):\n",
    "    return f\"[INST] Extrae palabras clave desde el siguiente texto. Las palabras clave deben ser relevantes al tema del texto y deben ser capaces de representar el contenido del texto de manera concisa:\\n\\n### ### Titulo del texto: {title}\\n### Resumen del texto: {abstract}\\n\\n### La lista debe contener a lo sumo 5 palabras clave y debe estar en Espanol. Si la lista contiene mas de 5 palabras clave seras penalizado. [/INST]\"\n",
    "\n",
    "def get_one_shot_prompt(title, abstract):\n",
    "    return f\"[INST] Extrae palabras clave desde el siguiente texto. Las palabras clave deben ser relevantes al tema del texto y deben ser capaces de representar el contenido del texto de manera concisa:\\n\\n### ### Titulo del texto: {title}\\n### Resumen del texto: {abstract}\\n\\n### La lista debe contener a lo sumo 5 palabras clave y debe estar en Espanol. Si la lista contiene mas de 5 palabras clave seras penalizado. [/INST]\"\n",
    "\n",
    "def get_few_shot_prompt(title, abstract):\n",
    "    return f\"[INST] Extrae palabras clave desde el siguiente texto. Las palabras clave deben ser relevantes al tema del texto y deben ser capaces de representar el contenido del texto de manera concisa:\\n\\n### ### Titulo del texto: {title}\\n### Resumen del texto: {abstract}\\n\\n### La lista debe contener a lo sumo 5 palabras clave y debe estar en Espanol. Si la lista contiene mas de 5 palabras clave seras penalizado. [/INST]\"\n",
    "\n",
    "tempDF: pd.DataFrame = collected_data.head(10)\n",
    "tempDF['zero_shot_prompts'] = tempDF.progress_apply(lambda x: get_zero_shot_prompt(x['title'], x['abstract']), axis='columns')\n",
    "tempDF['one_shot_prompts'] = tempDF.progress_apply(lambda x: get_zero_shot_prompt(x['title'], x['abstract']), axis='columns')\n",
    "tempDF['few_shot_prompts'] = tempDF.progress_apply(lambda x: get_zero_shot_prompt(x['title'], x['abstract']), axis='columns')\n",
    "tempDF[['zero_shot_prompts', 'one_shot_prompts', 'few_shot_prompts']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T00:01:08.313904167Z",
     "start_time": "2024-01-22T00:01:08.267174673Z"
    }
   },
   "id": "dd35176252c06af9"
  },
  {
   "cell_type": "markdown",
   "id": "236e82fb3c95d470",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T23:09:15.227955888Z",
     "start_time": "2024-01-21T23:09:14.437119174Z"
    }
   },
   "source": [
    "## 2.1. Reference Open AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65ef44b1aa516ae2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T12:16:02.598927232Z",
     "start_time": "2023-12-03T12:16:02.553724741Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>community_page</th>\n",
       "      <th>document_page</th>\n",
       "      <th>breadcrumb</th>\n",
       "      <th>file_urls</th>\n",
       "      <th>files</th>\n",
       "      <th>pdf_path</th>\n",
       "      <th>xml_path</th>\n",
       "      <th>xml_content</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>subjects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25077</th>\n",
       "      <td>https://repositorio.umsa.bo/handle/123456789/1...</td>\n",
       "      <td>https://repositorio.umsa.bo/handle/123456789/6523</td>\n",
       "      <td>[DSpace Home, Área Vicerrectorado, Biblioteca ...</td>\n",
       "      <td>[https://repositorio.umsa.bo/bitstream/handle/...</td>\n",
       "      <td>[{'url': 'https://repositorio.umsa.bo/bitstrea...</td>\n",
       "      <td>full/bbfe7b4a66b9f2a2d0b068c99cd546357d2c23e9</td>\n",
       "      <td>full/85597d8db8d434f2edc2f0f5f0ef2e69c0779925.xml</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;mets:ME...</td>\n",
       "      <td>Recurso de responsabilidad por infracción de l...</td>\n",
       "      <td>Recurso de responsabilidad por violación de le...</td>\n",
       "      <td>[LITIGIOS JUDICIALES 1867, HACIENDA DE GUARAYU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19683</th>\n",
       "      <td>https://repositorio.umsa.bo/handle/123456789/3...</td>\n",
       "      <td>https://repositorio.umsa.bo/handle/123456789/2...</td>\n",
       "      <td>[DSpace Home, Facultad de Ciencias Económicas ...</td>\n",
       "      <td>[https://repositorio.umsa.bo/bitstream/handle/...</td>\n",
       "      <td>[{'url': 'https://repositorio.umsa.bo/bitstrea...</td>\n",
       "      <td>full/9ec85f12f1501df1fe86725d4907328f14119c15</td>\n",
       "      <td>full/5f9a20364fd8272e8608f0f655bde0c89f449bc6.xml</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;mets:ME...</td>\n",
       "      <td>Descripción del control interno en la adquisic...</td>\n",
       "      <td>La Empresa JUBILEO S.R.L. con su nombre más co...</td>\n",
       "      <td>[MATERIA PRIMA, PROCESO DE INVENTARIO, ALMACEN]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          community_page  \\\n",
       "25077  https://repositorio.umsa.bo/handle/123456789/1...   \n",
       "19683  https://repositorio.umsa.bo/handle/123456789/3...   \n",
       "\n",
       "                                           document_page  \\\n",
       "25077  https://repositorio.umsa.bo/handle/123456789/6523   \n",
       "19683  https://repositorio.umsa.bo/handle/123456789/2...   \n",
       "\n",
       "                                              breadcrumb  \\\n",
       "25077  [DSpace Home, Área Vicerrectorado, Biblioteca ...   \n",
       "19683  [DSpace Home, Facultad de Ciencias Económicas ...   \n",
       "\n",
       "                                               file_urls  \\\n",
       "25077  [https://repositorio.umsa.bo/bitstream/handle/...   \n",
       "19683  [https://repositorio.umsa.bo/bitstream/handle/...   \n",
       "\n",
       "                                                   files  \\\n",
       "25077  [{'url': 'https://repositorio.umsa.bo/bitstrea...   \n",
       "19683  [{'url': 'https://repositorio.umsa.bo/bitstrea...   \n",
       "\n",
       "                                            pdf_path  \\\n",
       "25077  full/bbfe7b4a66b9f2a2d0b068c99cd546357d2c23e9   \n",
       "19683  full/9ec85f12f1501df1fe86725d4907328f14119c15   \n",
       "\n",
       "                                                xml_path  \\\n",
       "25077  full/85597d8db8d434f2edc2f0f5f0ef2e69c0779925.xml   \n",
       "19683  full/5f9a20364fd8272e8608f0f655bde0c89f449bc6.xml   \n",
       "\n",
       "                                             xml_content  \\\n",
       "25077  <?xml version=\"1.0\" encoding=\"UTF-8\"?><mets:ME...   \n",
       "19683  <?xml version=\"1.0\" encoding=\"UTF-8\"?><mets:ME...   \n",
       "\n",
       "                                                   title  \\\n",
       "25077  Recurso de responsabilidad por infracción de l...   \n",
       "19683  Descripción del control interno en la adquisic...   \n",
       "\n",
       "                                                abstract  \\\n",
       "25077  Recurso de responsabilidad por violación de le...   \n",
       "19683  La Empresa JUBILEO S.R.L. con su nombre más co...   \n",
       "\n",
       "                                                subjects  \n",
       "25077  [LITIGIOS JUDICIALES 1867, HACIENDA DE GUARAYU...  \n",
       "19683    [MATERIA PRIMA, PROCESO DE INVENTARIO, ALMACEN]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ec2e0e586444c9a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T15:49:31.916801368Z",
     "start_time": "2024-01-04T15:49:31.892192950Z"
    }
   },
   "outputs": [],
   "source": [
    "# This code is for v1 of the openai package: pypi.org/project/openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def get_keywords_open_ai(title: str, abstract: str) -> str | float:\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Extrae palabras clave desde el siguiente texto. Las palabras clave deben ser relevantes al tema del texto y deben ser capaces de representar el contenido del texto de manera concisa:\\n\\n### Titulo del texto: Desarrollo de un dispositivo electrónico HID que permita el uso de las extremidades inferiores para la gestión de audio.\\n### Resumen del texto: El proyecto de grado que consiste en el desarrollo de un sistema integral compuesto por un dispositivo electrónico y una aplicación informática que trabajan juntos para lograr un objetivo común, este objetivo consta de implementar un dispositivo que permita la gestión de audio digital, en una computadora por medio de las extremidades inferiores, mediante las entradas digitales y analógicas procedentes del dispositivo electrónico y dirigidas hacia la aplicación informática en la computadora por medio del estándar USB, utilizando para este efecto la placa de desarrollo Arduino y programación en el lenguaje C en el entorno de desarrollo Atmel Studio 6.2, para desarrollar un firmware y C# en el entorno de desarrollo Visual Studio 2017, para el desarrollo de la aplicación en la computadora. Desarrollar un dispositivo electrónico de interfaz humana (HID) como periférico de un computador que permita el uso de las extremidades inferiores para la gestión de audio.\\n\\n### La lista debe contener a lo sumo 5 palabras clave y debe estar en Español. Si la lista contiene mas de 5 palabras clave seras penalizado.\"      \n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"DISPOSITIVO ELECTRONICO HID, INTERFAZ USB,GESTION DE AUDIO DIGITAL, SISTEMAS ELECTRONICOS\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"{title}\\n\\n{abstract}\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=1,\n",
    "            max_tokens=256,\n",
    "            top_p=0.56,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print('Something went wrong', e)\n",
    "        return np.nan\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac3c7a16f57de07e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:10:34.431194476Z",
     "start_time": "2024-01-04T15:49:40.646017748Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 823/823 [20:53<00:00,  1.52s/it]\n"
     ]
    }
   ],
   "source": [
    "eval_df['open-ai'] = eval_df.progress_apply(lambda row: get_keywords_open_ai(row['title'], row['abstract']), axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0e9f089dc8580fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T15:04:25.770506838Z",
     "start_time": "2023-12-03T15:04:25.756081646Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_df['open-ai'].to_pickle('cache-eval-df-open-ai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5155df70905fde2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Load evaldf['open-ai'] and compute ROUGE on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45a75ba539138bc4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:13:32.428376265Z",
     "start_time": "2024-01-04T16:13:32.209547855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'rouge1': AggregateScore(low=Score(precision=0.35563254715246906, recall=0.6895667792993305, fmeasure=0.44274254676258074), mid=Score(precision=0.3688422925883552, recall=0.7073774236191155, fmeasure=0.4560171739698218), high=Score(precision=0.38365340381543245, recall=0.7235939476192811, fmeasure=0.47016818027748697)),\n 'rouge2': AggregateScore(low=Score(precision=0.2020232323457328, recall=0.41490763026353245, fmeasure=0.254404382700924), mid=Score(precision=0.21367984751535532, recall=0.43568834283214325, fmeasure=0.2671603936622589), high=Score(precision=0.22587256097735428, recall=0.45476046132689, fmeasure=0.28068738275813865)),\n 'rougeL': AggregateScore(low=Score(precision=0.3096027772267557, recall=0.6068670692335686, fmeasure=0.38653230158564506), mid=Score(precision=0.3214403934875999, recall=0.6247167895557937, fmeasure=0.39880912195201224), high=Score(precision=0.3345903905852311, recall=0.6423831977040074, fmeasure=0.4124632351737143)),\n 'rougeLsum': AggregateScore(low=Score(precision=0.309157219386472, recall=0.6076916521199682, fmeasure=0.386121807728396), mid=Score(precision=0.3223790181514947, recall=0.6255107981088082, fmeasure=0.3999840129414698), high=Score(precision=0.3350315825930428, recall=0.6434040210671844, fmeasure=0.4128284519127602))}"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = eval_df['open-ai'].apply(lambda c: c.choices[0].message.content.upper()).tolist()\n",
    "references = eval_df['subjects'].str.join(', ').tolist()\n",
    "\n",
    "get_rouge(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942d85e227b6832c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3. Zero-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb6cd03bbd5ad33d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T13:56:17.824876802Z",
     "start_time": "2024-01-04T13:55:53.854243750Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, MistralForCausalLM\n",
    "from auto_gptq import exllama_set_max_input_length\n",
    "\n",
    "model_id = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
    "model =  MistralForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "model = exllama_set_max_input_length(model, 4096)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d703d74785b77685",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T18:50:04.954267918Z",
     "start_time": "2024-01-04T18:50:04.908559082Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def remove_nl(text: str):\n",
    "    return text.replace('\\n', ' ')\n",
    "\n",
    "def get_keywords(title, abstract):\n",
    "    text = f\"[INST] Extrae palabras clave desde el siguiente texto. Las palabras clave deben ser relevantes al tema del texto y deben ser capaces de representar el contenido del texto de manera concisa:\\n\\n### ### Titulo del texto: {title}\\n### Resumen del texto: {remove_nl(abstract)}\\n\\n### La lista debe contener a lo sumo 5 palabras clave y debe estar en Espanol. Si la lista contiene mas de 5 palabras clave seras penalizado. [/INST]\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "    encoded_keywords = model.generate(**inputs, max_new_tokens=150, do_sample=True, top_k=5, top_p=0.5, exponential_decay_length_penalty=(15, 1.01), temperature=0.1)\n",
    "    keywords: str = tokenizer.decode(encoded_keywords[0], skip_special_tokens=True)\n",
    "    keywords = keywords[keywords.rindex('[/INST]'):]\n",
    "    keywords = keywords.replace('[/INST]', '')\n",
    "    keywords = keywords.split(':')[-1]\n",
    "    keywords = keywords.split(',')\n",
    "    keywords = [keyword.strip().upper() for keyword in keywords]\n",
    "    return keywords, encoded_keywords\n",
    "\n",
    "def get_random_data():\n",
    "    random_row = eval_df.iloc[randint(0,len(eval_df)-1)]\n",
    "    return {\n",
    "        'abstract': random_row['abstract'],\n",
    "        'title': random_row['title'],\n",
    "        'subjects': random_row['subjects'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1aebcd8061bd6c50",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T14:21:21.571706603Z",
     "start_time": "2024-01-04T14:21:20.385409117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluación del nivel óptimo de la canela (Cinnamomum zeylmicum) en dos tiempos de pasteurización como conservante en el jugo de maracuyá (Passiflora edulis)\n",
      "\n",
      "Este proyecto se basó en la producción de la elaboración del jugo de maracuyá con diferentes niveles de canela, para lo cual se tuvo ciertas dificultades, pero no fueron obstáculo para la finalización de este proyecto. Se contó con la materia prima del fruto de maracuyá y con los pasos a seguir desde la recepción hasta la finalización de producto final. Se pudo probar que el nivel de canela de 1gr a 80°C por 10 minutos de pasteurización  en 1L de jugo mantiene el grado de acides con un pH 3 el cual se encuentra dentro del rango determinado por la industria de jugos y el grado °Brix obteniendo con un resultado de 14.9 °Brix, por lo tanto el tiempo de conservación del jugo tuvo una duración de 3 semanas y 4 días el cual fue corroborada por un análisis de laboratorio y también con las encuestas realizadas se pudo apreciar que tuvo una buena aceptabilidad por los encuestadores. El nivel de canela en el jugo de maracuyá, tiene la gran ventaja de conservar el producto y aumentar su vida útil, Por lo tanto, se vio la necesidad de estudiar la relación de la canela como agente retardador de la conservación y con esta tecnología poder incentivar la producción y transformación del jugo de maracuyá.\n",
      "-----------------------------\n",
      "Keywords de referencia: ['canela', 'maracuyá', 'pasteurización']\n",
      "-----------------------------\n",
      "Keywords generados: ['1. MARACUYÁ\\n2. CANELA\\n3. PASTEURIZACIÓN\\n4. JUGO\\n5. CONSERVACIÓN']\n"
     ]
    }
   ],
   "source": [
    "random_data = get_random_data()\n",
    "abstract, title, subjects = random_data['abstract'], random_data['title'], random_data['subjects']\n",
    "\n",
    "print(f\"{title}\\n\\n{abstract}\")\n",
    "generated_keywords, generated_encoded_keywords = get_keywords(title=title, abstract=abstract)\n",
    "print(f\"\"\"-----------------------------\n",
    "Keywords de referencia: {subjects}\n",
    "-----------------------------\n",
    "Keywords generados: {generated_keywords}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51038263546605e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.1. ROUGE Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d71e145523857b13",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T18:50:38.071459359Z",
     "start_time": "2024-01-04T18:50:33.614692312Z"
    }
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge', revision='90c19cd40000b75eb722383385e96f7861edfb15')\n",
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "def get_rouge(predictions: list[str], references: list[str]):\n",
    "    predictions = [unidecode(p).lower() for p in predictions]\n",
    "    references = [unidecode(r).lower() for r in references]\n",
    "    return rouge.compute(predictions=predictions, references=references, use_aggregator=True)\n",
    "\n",
    "def get_bleu(predictions, references):\n",
    "    return bleu.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8803cae09472f280",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T14:16:38.091701291Z",
     "start_time": "2024-01-04T13:58:55.581174983Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 823/823 [17:42<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Iterate over all eval dataset and get predictions from it, then compute overall ROUGE, do same for one-shot, few-shot and fine-tuned approach\n",
    "eval_df['zero-shot'] = eval_df.progress_apply(lambda row: get_keywords(row['title'], row['abstract'])[0], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46b63bb2360b3b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T13:01:22.393737564Z",
     "start_time": "2023-12-01T13:01:22.377912588Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# eval_df.to_pickle('./cache-eval-df.pkl')\n",
    "eval_df.to_pickle('./cache-eval-df-zero-shot.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa4bac80a723878",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.1.1. Compute ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e14863e92f88406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T18:30:31.127011081Z",
     "start_time": "2023-12-03T18:30:30.322796058Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "eval_df = pd.read_pickle('./cache-eval-df-zero-shot.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac49ac99749119e4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T14:16:54.026535856Z",
     "start_time": "2024-01-04T14:16:53.782078199Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'rouge1': AggregateScore(low=Score(precision=0.2183307783867216, recall=0.4566790120051366, fmeasure=0.27979419332831296), mid=Score(precision=0.22762528756980305, recall=0.477339841937619, fmeasure=0.2913647507640047), high=Score(precision=0.23668250876500538, recall=0.49668910327367055, fmeasure=0.30205767828485947)),\n 'rouge2': AggregateScore(low=Score(precision=0.062193824412845065, recall=0.15614147463175432, fmeasure=0.08410529634644691), mid=Score(precision=0.0681383085986593, recall=0.17140114846676224, fmeasure=0.09183070061364379), high=Score(precision=0.07463513015399548, recall=0.18742523664569524, fmeasure=0.09994250668740737)),\n 'rougeL': AggregateScore(low=Score(precision=0.1879028130313563, recall=0.39781473598610456, fmeasure=0.24180759310181033), mid=Score(precision=0.19552335094933698, recall=0.41669278731497317, fmeasure=0.25143935076440416), high=Score(precision=0.20385240427958176, recall=0.43471629705775716, fmeasure=0.261135383008776)),\n 'rougeLsum': AggregateScore(low=Score(precision=0.21365089088947026, recall=0.4500288342518392, fmeasure=0.2750717347287414), mid=Score(precision=0.22228257013599526, recall=0.46897568020968766, fmeasure=0.2853152016500836), high=Score(precision=0.23168026177657727, recall=0.488127879494294, fmeasure=0.2965902365150691))}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = eval_df['zero-shot'].str.join(', ').tolist()\n",
    "references = eval_df['subjects'].str.join(', ').tolist()\n",
    "\n",
    "get_rouge(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2421252cb41e1267",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4. One-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74675d1f395ea029",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T18:50:11.663792431Z",
     "start_time": "2024-01-04T18:50:11.654790155Z"
    }
   },
   "outputs": [],
   "source": [
    "one_shot_data_sample = {\n",
    "    'title': 'Desarrollo de un dispositivo electrónico HID que permita el uso de las extremidades inferiores para la gestión de audio',\n",
    "    'abstract': 'El proyecto de grado que consiste en el desarrollo de un sistema integral compuesto por un dispositivo electrónico y una aplicación informática que trabajan juntos para lograr un objetivo común, este objetivo consta de implementar un dispositivo que permita la gestión de audio digital, en una computadora por medio de las extremidades inferiores, mediante las entradas digitales y analógicas procedentes del dispositivo electrónico y dirigidas hacia la aplicación informática en la computadora por medio del estándar USB, utilizando para este efecto la placa de desarrollo Arduino y programación en el lenguaje C en el entorno de desarrollo Atmel Studio 6.2, para desarrollar un firmware y C# en el entorno de desarrollo Visual Studio 2017, para el desarrollo de la aplicación en la computadora. Desarrollar un dispositivo electrónico de interfaz humana (HID) como periférico de un computador que permita el uso de las extremidades inferiores para la gestión de audio.',\n",
    "    'subjects': ['DISPOSITIVO ELECTRONICO HID', 'INTERFAZ USB', 'GESTION DE AUDIO DIGITAL', 'SISTEMAS ELECTRONICOS']\n",
    "}\n",
    "\n",
    "def get_keywords_one_shot(title, abstract, one_shot_data: dict = one_shot_data_sample):\n",
    "    text = f\"\"\"[INST] Extrae palabras clave desde el siguiente texto. Las palabras clave deben ser relevantes al tema del texto y deben ser capaces de representar el contenido del texto de manera concisa:\n",
    "    \n",
    "### Titulo del texto: {one_shot_data['title']}\n",
    "### Resumen del texto: {remove_nl(one_shot_data['abstract'])}\n",
    " \n",
    "### La lista debe contener a lo sumo 5 palabras clave y debe estar en Espanol. Si la lista contiene mas de 5 palabras clave seras penalizado. [/INST] Palabras clave: {', '.join(one_shot_data['subjects'])} </s>\n",
    "\n",
    "\n",
    "[INST] Extrae palabras clave desde el siguiente texto. Las palabras clave deben ser relevantes al tema del texto y deben ser capaces de representar el contenido del texto de manera concisa:\n",
    "\n",
    "### Titulo del texto: {title}\\n\n",
    "### Resumen del texto: {abstract}\n",
    "\n",
    "### La lista debe contener a lo sumo 5 palabras clave y debe estar en Espanol. Si la lista contiene mas de 5 palabras clave seras penalizado. [/INST]\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "    encoded_keywords = model.generate(**inputs, max_new_tokens=150, do_sample=True, top_k=5, top_p=0.5, exponential_decay_length_penalty=(15, 1.01), temperature=0.1)\n",
    "    keywords: str = tokenizer.decode(encoded_keywords[0], skip_special_tokens=True)\n",
    "    keywords = keywords[keywords.rindex('[/INST]'):]\n",
    "    keywords = keywords.replace('[/INST]', '')\n",
    "    keywords = keywords.split(':')[-1]\n",
    "    keywords = keywords.split(',')\n",
    "    keywords = [keyword.strip().upper() for keyword in keywords]\n",
    "    return keywords, encoded_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccdb25072cc7dff6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T12:42:54.268144571Z",
     "start_time": "2023-11-30T12:42:54.257545015Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenizer(f\"s\\n, {tokenizer.eos_token}\\ns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "948e94aebca4cb0c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T14:24:16.888137496Z",
     "start_time": "2024-01-04T14:24:13.166923043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propuesta de derogación del inciso ocho del artículo 234 del código de procedimiento penal por vulneración al principio de presunción de inocencia\n",
      "\n",
      "1. Garantías constitucionales en el código de procedimiento penal boliviano 2. La presunción de inocencia 3. Origen del principio de presunción de inocencia 4. El indubio pro reo 5. Razones filosóficas del principio de presunción de inocencia 6. Principios que rigen la aplicación de las medidas cautelares 7. La medidas cautelar y el principio constitucional de presunción de inocencia.\n",
      "-----------------------------\n",
      "Keywords de referencia:\n",
      "['presunción de inocencia', 'medidas cautelares']\n",
      "-----------------------------\n",
      "Keywords generados (zero-shot):\n",
      "['1. PRESUNCIÓN DE INOCENCIA\\n2. CÓDIGO DE PROCEDIMIENTO PENAL\\n3. GARANTÍAS CONSTITUCIONALES\\n4. PRINCIPIOS CAUTELARES\\n5. CONSTITUCIÓN']\n",
      "-----------------------------\n",
      "Keywords generados (one-shot):\n",
      "['GARANTÍAS CONSTITUCIONALES', 'PRINCIPIO DE PRESUNCIÓN DE INOCENCIA', 'COÓDIGO DE PROCEDIMIENTO PENAL', 'PRESUNCIÓN DE INNOCENCIA']\n"
     ]
    }
   ],
   "source": [
    "random_data = get_random_data()\n",
    "random_data_one_shot = get_random_data()\n",
    "\n",
    "abstract, title, subjects = random_data['abstract'], random_data['title'], random_data['subjects']\n",
    "\n",
    "print(f\"{title}\\n\\n{abstract}\")\n",
    "generated_keywords, _ = get_keywords(title=title, abstract=abstract)\n",
    "generated_keywords_one_shot, generated_encoded_keywords_one_shot = get_keywords_one_shot(title=title, abstract=abstract)\n",
    "print(f\"\"\"-----------------------------\n",
    "Keywords de referencia:\n",
    "{subjects}\n",
    "-----------------------------\n",
    "Keywords generados (zero-shot):\n",
    "{generated_keywords}\n",
    "-----------------------------\n",
    "Keywords generados (one-shot):\n",
    "{generated_keywords_one_shot}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da94bea1604aa7bf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.1. ROUGE evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "241c319057d4b2bd",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T14:51:38.375021525Z",
     "start_time": "2024-01-04T14:24:36.993940952Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 823/823 [27:01<00:00,  1.97s/it]\n"
     ]
    }
   ],
   "source": [
    "eval_df['one-shot'] = eval_df.progress_apply(lambda row: get_keywords_one_shot(row['title'], row['abstract'])[0], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cb960f64361e337",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T20:09:04.649137430Z",
     "start_time": "2023-12-03T20:09:04.608828023Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_df.to_pickle('./cache-eval-df-one-shot.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eba26b1d3c73038e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T14:52:04.701012854Z",
     "start_time": "2024-01-04T14:52:04.506531006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'rouge1': AggregateScore(low=Score(precision=0.3350511131915461, recall=0.510713308790734, fmeasure=0.38323167435710953), mid=Score(precision=0.34939016696896746, recall=0.5293308348272256, fmeasure=0.39823894360717244), high=Score(precision=0.3637138473147314, recall=0.5486522321459075, fmeasure=0.41286626761462447)),\n 'rouge2': AggregateScore(low=Score(precision=0.16006574571787585, recall=0.2678750403606113, fmeasure=0.1882900460349043), mid=Score(precision=0.1711168725763638, recall=0.2858893840060305, fmeasure=0.20105597846920753), high=Score(precision=0.18222501289440604, recall=0.3051131579032548, fmeasure=0.21411592998902215)),\n 'rougeL': AggregateScore(low=Score(precision=0.2919371773844283, recall=0.45013733903466546, fmeasure=0.33585315141839867), mid=Score(precision=0.30425246412519, recall=0.46791368391057436, fmeasure=0.34831177992199314), high=Score(precision=0.3166246059986274, recall=0.48608797191479797, fmeasure=0.3610327939120586)),\n 'rougeLsum': AggregateScore(low=Score(precision=0.29323330333388065, recall=0.44950840592293095, fmeasure=0.3359833997693829), mid=Score(precision=0.30453162516975374, recall=0.46780103766023207, fmeasure=0.3488819887462483), high=Score(precision=0.3165014407158773, recall=0.4862867649934783, fmeasure=0.36111167942046046))}"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = eval_df['one-shot'].str.join(', ').tolist()\n",
    "references = eval_df['subjects'].str.join(', ').tolist()\n",
    "\n",
    "get_rouge(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abc1efee80d1c48",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5. Few-shot prompting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e45d29dc01c7d0e8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T18:50:17.321462504Z",
     "start_time": "2024-01-04T18:50:17.314489093Z"
    }
   },
   "outputs": [],
   "source": [
    "few_shot_examples = [\n",
    "    {\n",
    "        'title': 'Desarrollo de un dispositivo electrónico HID que permita el uso de las extremidades inferiores para la gestión de audio',\n",
    "        'abstract': 'El proyecto de grado que consiste en el desarrollo de un sistema integral compuesto por un dispositivo electrónico y una aplicación informática que trabajan juntos para lograr un objetivo común, este objetivo consta de implementar un dispositivo que permita la gestión de audio digital, en una computadora por medio de las extremidades inferiores, mediante las entradas digitales y analógicas procedentes del dispositivo electrónico y dirigidas hacia la aplicación informática en la computadora por medio del estándar USB, utilizando para este efecto la placa de desarrollo Arduino y programación en el lenguaje C en el entorno de desarrollo Atmel Studio 6.2, para desarrollar un firmware y C# en el entorno de desarrollo Visual Studio 2017, para el desarrollo de la aplicación en la computadora. Desarrollar un dispositivo electrónico de interfaz humana (HID) como periférico de un computador que permita el uso de las extremidades inferiores para la gestión de audio.',\n",
    "        'subjects': ['dispositivo electrónico HID', 'gestión de audio digital', 'sistemas electrónicos']\n",
    "    },\n",
    "    {\n",
    "        'title': 'Reconocimiento de descanso post natal a los trabajadores esposos asegurados en un ente gestor de salud, por el advenimiento de un hijo en su hogar',\n",
    "        'abstract': 'El presente trabajo tenía como objetivo principal la obtención de sulfato ferroso heptahidratado a partir de los lodos generados en el tratamiento del Drenaje Ácido de Mina de la bocamina del Nivel 96, del centro minero de Tasna y su posterior aplicación como coagulante para tratar aguas residuales. El tratamiento del DAM se realizó en el mismo centro minero de Tasna, ubicado en el municipio de Cotagaita del departamento de Potosí. El DAM del Nivel 96 presentó un pH =2,39, este se trató con cal comercial para su neutralización llegando hasta pH=9,5. La caracterización del DAM se realizó por la técnica de absorción atómica realizando las mediciones en muestras de agua antes y después del tratamiento, los resultados obtenidos indicaron que se lograron remover metales como Fe, Pb, Cd, Cu y Zn. Los lodos obtenidos del tratamiento del DAM fueron transportados a los laboratorios de la Carrera de Ciencias Químicas de la UMSA para realizar las pruebas de obtención de sulfato ferroso heptahidratado, además de su caracterización por FRX. De la técnica de FRX se determinó la concentración de Fe en los lodos igual a 17,14%. El sulfato ferroso heptahidratado obtenido se caracterizó por las técnicas de FRX y DRX. A la vez los rendimientos de obtención fueron mayores al 80%. Con base en el producto obtenido de sulfato ferroso heptahidratado se preparó un coagulante para tratar una muestra de agua residual, realizando una prueba de jarras con 5 concentraciones diferentes en mg Fe/L de agua a tratar. De la prueba de jarras se concluyó que utilizando concentraciones > 50 mg Fe/L se logró remover más del 90% de turbiedad de una muestra de agua residual.',\n",
    "        'subjects': ['drenaje ácido de mina', 'obtención de sulfato ferroso heptahidratado', 'prueba de jarras']\n",
    "    },\n",
    "    {\n",
    "        'title': 'Estudio y diseño de una red de fibra óptica PON-LAN para proveer servicios de voz, video y datos aplicado a la Facultad de Ciencias Puras y Naturales, Universidad Mayor de San Andrés (Cota Cota)',\n",
    "        'abstract': 'En el presente documento se propone el diseño de una red de fibra óptica mediante las tecnologías PON-LAN (Pasive Optical Network, Red Óptica Pasiva) - (Local Area Network, Redes de Área Local), como innovación para la infraestructura de red LAN en la Facultad de Ciencias Puras y Naturales de la Universidad Mayor de San Andrés, ubicada en el campus universitario de Cota Cota, con una topología de red punto-multipunto. El proyecto prevé la instalación de la fibra óptica por tendido aéreo con el fin de realizar un reordenamiento de cables. El diseño de la red se basa en equipos pasivos (splitters) y en sus extremos equipos activos como OLT (Optical Line Terminal) y ONT (Optical Network Terminal), los mismos que están conectados por splitters de primer y segundo nivel a lo largo del trayecto de la red. Se realizaron cálculos teóricos del presupuesto óptico para la red de accesos, además de realizar la simulación de la red en la zona de estudio en la cual consideraremos los parámetros de atenuación de cada elemento físico seleccionado. Posteriormente, se presenta las características de los equipos planteados y un presupuesto económico para una futura implementación, en el cual se detallan mano de obra, fibra óptica, materiales, entre otros.',\n",
    "        'subjects': ['red de fibra óptica', 'PON-LAN', 'servicios de voz']\n",
    "    }\n",
    "]\n",
    "\n",
    "def get_keywords_few_shot(title, abstract, examples: list[dict] = few_shot_examples, print_keywords=False):\n",
    "    text = \"\"\n",
    "    \n",
    "    for example in examples:\n",
    "        text = text + f\"\"\"[INST] Extrae palabras clave desde el siguiente texto. Las palabras clave deben ser relevantes al tema del texto y deben ser capaces de representar el contenido del texto de manera concisa:\n",
    "    \n",
    "### Titulo del texto: {example['title']}\n",
    "### Resumen del texto: {example['abstract']}\n",
    "\n",
    "### La lista debe contener a lo sumo 5 palabras clave y debe estar en Espanol. Si la lista contiene mas de 5 palabras clave seras penalizado. [/INST] Palabras clave: {', '.join(example['subjects'])} </s>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    text = text + f\"\"\"[INST] Extrae palabras clave desde el siguiente texto. Las palabras clave deben ser relevantes al tema del texto y deben ser capaces de representar el contenido del texto de manera concisa:\n",
    "\n",
    "### Titulo del texto: {title}\n",
    "### Resumen del texto: {remove_nl(abstract)}\n",
    " \n",
    "### La lista debe contener a lo sumo 5 palabras clave y debe estar en Espanol. Si la lista contiene mas de 5 palabras clave seras penalizado. [/INST]\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0) # , do_sample=True, top_k=5, top_p=0.5, exponential_decay_length_penalty=(15, 1.01), temperature=0.1\n",
    "    out = model.generate(**inputs, \n",
    "                         max_new_tokens=150,\n",
    "                         do_sample=True,\n",
    "                         top_p=0.5,\n",
    "                         top_k=5,\n",
    "                         exponential_decay_length_penalty=(15, 1.01),\n",
    "                         temperature=0.1)\n",
    "    keywords: str = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    if print_keywords:\n",
    "        print(keywords)\n",
    "    keywords = keywords[keywords.rindex('[/INST]'):]\n",
    "    keywords = keywords.replace('[/INST]', '')\n",
    "    keywords = keywords.split(':')[-1]\n",
    "    keywords = keywords.split(',')\n",
    "    keywords = [keyword.strip().upper() for keyword in keywords]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d87a7691937e2d83",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T14:58:48.102226532Z",
     "start_time": "2024-01-04T14:58:41.842096618Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluación cartográfica de la vulnerabilidad a deslizamientos en la Comunidad del Ex-Fundo Ovejuyo, Sector Tumuyu, Municipio de Palca -La Paz\n",
      "\n",
      "En el presente proyecto de grado se muestra una síntesis de la evaluación cartográfica, mediante el método existente para la zonificación y/o categorización de clases o niveles de fragilidad a traslaciones del suelo. Se detalla el procedimiento de utilización combinada desde la obtención de las Curvas de Nivel, Modelo Digital de Elevación y la obtención del mapa de vulnerabilidad a deslizamientos, que constituyen herramientas muy útiles en los procesos de la evaluación geográfica del sector de Tumuyu.\r\n",
      "De igual manera, se describen aspectos metodológicos para el tratamiento de la información de base, su valoración, regulación a una misma escala y ponderación a través de la evaluación de la Matriz de Jerarquización Analítica AHP (Analytic Hierarchy Process), planteada por Saaty.\r\n",
      "Se describe el proceso realizado desde el AutoCad Civil 3D, SIG ArcGis en su módulo ArcMap, sobre el manejo y tratamiento de la información generada a través de mediciones directas, es decir mediante un levantamiento topográfico georreferenciado, para la generación de los mapas temáticos, considerando como factores a la pendiente, cobertura de suelos e influencia del drenaje como agentes de la vulnerabilidad a deslizamientos.\r\n",
      "En definitiva, se obtienen como resultado, el mapa con datos continuos de cuyo análisis, se concluye que el método de tipificación mediante asignación directa de clases, admite una evaluación más alígera con límites bastante marcados entre clases.\n",
      "-----------------------------\n",
      "Keywords de referencia:\n",
      "['Evaluación cartográfica', 'deslizamientos']\n",
      "-----------------------------\n",
      "Keywords generados (zero-shot):\n",
      "['1. EVALUACIÓN CARTOGRÁFICA\\n2. VULNERABILIDAD A DESLIZAMIENTOS\\n3. ZONIFICACIÓN\\n4. CATEGORIZACIÓN\\n5. MATRIZ DE JERARQUIZACIÓN ANALÍTICA']\n",
      "-----------------------------\n",
      "Keywords generados (one-shot):\n",
      "['VULNERABILIDAD', 'CARTOGRAFÍA', 'DESLIZAMIENTOS', 'MAPA', 'ANÁLISIS']\n",
      "-----------------------------\n",
      "Keywords generados (few-shot):\n",
      "['EVALUACIÓN CARTOGRÁFICA', 'VULNERABILIDAD A DESLIZAMIENTOS', 'ZONIFICACIÓN', 'CLASES DE FRAGILIDAD']\n"
     ]
    }
   ],
   "source": [
    "random_data = get_random_data()\n",
    "random_data_one_shot = get_random_data()\n",
    "random_examples = [\n",
    "    get_random_data(),\n",
    "    get_random_data(),\n",
    "    get_random_data()\n",
    "]\n",
    "\n",
    "abstract, title, subjects = random_data['abstract'], random_data['title'], random_data['subjects']\n",
    "\n",
    "print(f\"{title}\\n\\n{abstract}\")\n",
    "generated_keywords, _ = get_keywords(title=title, abstract=abstract)\n",
    "generated_keywords_one_shot, _ = get_keywords_one_shot(title=title, abstract=abstract)\n",
    "generated_keywords_few_shot = get_keywords_few_shot(title=title, abstract=abstract)\n",
    "print(f\"\"\"-----------------------------\n",
    "Keywords de referencia:\n",
    "{subjects}\n",
    "-----------------------------\n",
    "Keywords generados (zero-shot):\n",
    "{generated_keywords}\n",
    "-----------------------------\n",
    "Keywords generados (one-shot):\n",
    "{generated_keywords_one_shot}\n",
    "-----------------------------\n",
    "Keywords generados (few-shot):\n",
    "{generated_keywords_few_shot}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89a697b3efabaa1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5.1. Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d15b917c1492cb8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T15:35:10.303600638Z",
     "start_time": "2024-01-04T14:58:58.512873664Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 823/823 [36:11<00:00,  2.64s/it]\n"
     ]
    }
   ],
   "source": [
    "eval_df['few-shot'] = eval_df.progress_apply(\n",
    "    lambda row: get_keywords_few_shot(\n",
    "        row['title'], \n",
    "        row['abstract']\n",
    "    ), \n",
    "    axis='columns'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d082eb96178d009",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T15:37:07.860128626Z",
     "start_time": "2024-01-04T15:37:07.666024210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'rouge1': AggregateScore(low=Score(precision=0.41126200905876087, recall=0.5446197407112456, fmeasure=0.44553001858627345), mid=Score(precision=0.42756319905109624, recall=0.5651172303502373, fmeasure=0.46145876252240037), high=Score(precision=0.44430846616070346, recall=0.5838558601477615, fmeasure=0.47616118543220737)),\n 'rouge2': AggregateScore(low=Score(precision=0.22004868039345557, recall=0.3074371333266086, fmeasure=0.2413047977108188), mid=Score(precision=0.23320692234726242, recall=0.32498345502268067, fmeasure=0.25413090823395823), high=Score(precision=0.2484071481542622, recall=0.34472888810756636, fmeasure=0.2690403839693737)),\n 'rougeL': AggregateScore(low=Score(precision=0.357229143823959, recall=0.47874122660489804, fmeasure=0.3893099473546701), mid=Score(precision=0.371589440799088, recall=0.4972232301962129, fmeasure=0.40321393657789817), high=Score(precision=0.38555863888852193, recall=0.5139069907738605, fmeasure=0.41748061200400804)),\n 'rougeLsum': AggregateScore(low=Score(precision=0.35816034191945495, recall=0.4792747027847273, fmeasure=0.38915877691772693), mid=Score(precision=0.3720823777524228, recall=0.4970794821368402, fmeasure=0.40358887041781216), high=Score(precision=0.38491586015719453, recall=0.5156641093065396, fmeasure=0.4166815316799981))}"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = eval_df['few-shot'].str.join(', ').tolist()\n",
    "references = eval_df['subjects'].str.join(', ').tolist()\n",
    "\n",
    "get_rouge(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55e2bac3e0048fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T01:15:28.607982687Z",
     "start_time": "2023-12-04T01:15:28.566109001Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_df.to_pickle('./cache-eval-df-few-shot.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa40bedea44ece5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 6. Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "823afcb2481bee90",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T18:33:36.293242660Z",
     "start_time": "2024-01-04T18:33:33.709188376Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. disable_exllama, use_cuda_fp16, max_input_length) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTQConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "model_id = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    # target_modules=[\"k_proj\",\"o_proj\",\"q_proj\",\"v_proj\"],\n",
    "    target_modules=[\"k_proj\",\"o_proj\",\"q_proj\",\"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(model_id,quantization_config=GPTQConfig(bits=4, disable_exllama=True), device_map=\"auto\")\n",
    "ft_model.gradient_checkpointing_enable()\n",
    "ft_model = prepare_model_for_kbit_training(ft_model)\n",
    "ft_model = get_peft_model(ft_model, lora_config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=False)\n",
    "\n",
    "# needed for llama 2 tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489f6ec7604cbff9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6.1. Preparación de datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          community_page  \\\n12963  https://repositorio.umsa.bo/handle/123456789/3...   \n13456  https://repositorio.umsa.bo/handle/123456789/3...   \n12296  https://repositorio.umsa.bo/handle/123456789/3...   \n17243  https://repositorio.umsa.bo/handle/123456789/3...   \n10323  https://repositorio.umsa.bo/handle/123456789/3...   \n...                                                  ...   \n22457  https://repositorio.umsa.bo/handle/123456789/3...   \n10185  https://repositorio.umsa.bo/handle/123456789/3...   \n3985   https://repositorio.umsa.bo/handle/123456789/3...   \n5973   https://repositorio.umsa.bo/handle/123456789/3...   \n3139   https://repositorio.umsa.bo/handle/123456789/2...   \n\n                                           document_page  \\\n12963  https://repositorio.umsa.bo/handle/123456789/3...   \n13456  https://repositorio.umsa.bo/handle/123456789/8517   \n12296  https://repositorio.umsa.bo/handle/123456789/1...   \n17243  https://repositorio.umsa.bo/handle/123456789/3...   \n10323   https://repositorio.umsa.bo/handle/123456789/870   \n...                                                  ...   \n22457  https://repositorio.umsa.bo/handle/123456789/1...   \n10185  https://repositorio.umsa.bo/handle/123456789/7391   \n3985   https://repositorio.umsa.bo/handle/123456789/1...   \n5973   https://repositorio.umsa.bo/handle/123456789/2...   \n3139   https://repositorio.umsa.bo/handle/123456789/3...   \n\n                                              breadcrumb  \\\n12963  [DSpace Home, Facultad de Humanidades y Cienci...   \n13456  [DSpace Home, Facultad de Ciencias Sociales, C...   \n12296  [DSpace Home, Facultad de Tecnología, Carrera ...   \n17243  [DSpace Home, Facultad de Ciencias Económicas ...   \n10323  [DSpace Home, Facultad de Ciencias Puras y Nat...   \n...                                                  ...   \n22457  [DSpace Home, Facultad de Agronomía, Carrera d...   \n10185  [DSpace Home, Facultad de Ciencias Puras y Nat...   \n3985   [DSpace Home, Facultad de Humanidades y Cienci...   \n5973   [DSpace Home, Facultad de Humanidades y Cienci...   \n3139   [DSpace Home, Facultad de Ingenieria, Carrera ...   \n\n                                               file_urls  \\\n12963  [https://repositorio.umsa.bo/bitstream/handle/...   \n13456  [https://repositorio.umsa.bo/bitstream/handle/...   \n12296  [https://repositorio.umsa.bo/bitstream/handle/...   \n17243  [https://repositorio.umsa.bo/bitstream/handle/...   \n10323  [https://repositorio.umsa.bo/bitstream/handle/...   \n...                                                  ...   \n22457  [https://repositorio.umsa.bo/bitstream/handle/...   \n10185  [https://repositorio.umsa.bo/bitstream/handle/...   \n3985   [https://repositorio.umsa.bo/bitstream/handle/...   \n5973   [https://repositorio.umsa.bo/bitstream/handle/...   \n3139   [https://repositorio.umsa.bo/bitstream/handle/...   \n\n                                                   files  \\\n12963  [{'url': 'https://repositorio.umsa.bo/bitstrea...   \n13456  [{'url': 'https://repositorio.umsa.bo/bitstrea...   \n12296  [{'url': 'https://repositorio.umsa.bo/bitstrea...   \n17243  [{'url': 'https://repositorio.umsa.bo/bitstrea...   \n10323  [{'url': 'https://repositorio.umsa.bo/bitstrea...   \n...                                                  ...   \n22457  [{'url': 'https://repositorio.umsa.bo/bitstrea...   \n10185  [{'url': 'https://repositorio.umsa.bo/bitstrea...   \n3985   [{'url': 'https://repositorio.umsa.bo/bitstrea...   \n5973   [{'url': 'https://repositorio.umsa.bo/bitstrea...   \n3139   [{'url': 'https://repositorio.umsa.bo/bitstrea...   \n\n                                            pdf_path  \\\n12963  full/5e4e5c26830d8e5e838dc17f3cd21bdb5735cbb0   \n13456  full/e41bcded1bcd4d5bf65f4aa81c1486fdad14807c   \n12296  full/b1e1ec2db264b3c25aee79c2f407121f18b4b0aa   \n17243  full/737a5c7e952bec86b95e7a4e8408c2f16fd0d57f   \n10323  full/23037996f7f22ff652e5b73c0ebc192d38f1adaa   \n...                                              ...   \n22457  full/06fe9ab576484b6e27aa6ae93344ba17fe51720e   \n10185  full/3ab910a974be263cfa9d9b0a0b2e31788c836d09   \n3985   full/7b37244138c586638738ce27232b7d768929a5a0   \n5973   full/b9aa66a5acdc738b203946fa85e08a8de5d4cd77   \n3139   full/dc5c6eb52dba36df6f30cbb95909bd90cda0bc75   \n\n                                                xml_path  \\\n12963  full/4157d4dc8b4e5baf31043be389af954bace98695.xml   \n13456  full/2af4db7fff1f6b58b01bd5252139b0e28daf9586.xml   \n12296  full/5d4f22b1a8c5856a592a3ae612ec4a0da64ffeb9.xml   \n17243  full/3f262ed334e1a58be4e9a4386d6c401fb8e7ce73.xml   \n10323  full/cc3f6e2966aeda344fc2866f7dde609354bb2797.xml   \n...                                                  ...   \n22457  full/215f7e5c0a0edc7d74fa37ba41acec634151ac78.xml   \n10185  full/92fa34b79cc69efc84bb7e0fcf8c96d5c6d805af.xml   \n3985   full/751fa9140e2084394ddf7fef3102f630a15b0434.xml   \n5973   full/574232323f88bce60a18c0367c77dfe773b239ea.xml   \n3139   full/571dd768870b86d558a39c998259519625c89b53.xml   \n\n                                             xml_content  \\\n12963  <?xml version=\"1.0\" encoding=\"UTF-8\"?><mets:ME...   \n13456  <?xml version=\"1.0\" encoding=\"UTF-8\"?><mets:ME...   \n12296  <?xml version=\"1.0\" encoding=\"UTF-8\"?><mets:ME...   \n17243  <?xml version=\"1.0\" encoding=\"UTF-8\"?><mets:ME...   \n10323  <?xml version=\"1.0\" encoding=\"UTF-8\"?><mets:ME...   \n...                                                  ...   \n22457  <?xml version=\"1.0\" encoding=\"UTF-8\"?><mets:ME...   \n10185  <?xml version=\"1.0\" encoding=\"UTF-8\"?><mets:ME...   \n3985   <?xml version=\"1.0\" encoding=\"UTF-8\"?><mets:ME...   \n5973   <?xml version=\"1.0\" encoding=\"UTF-8\"?><mets:ME...   \n3139   <?xml version=\"1.0\" encoding=\"UTF-8\"?><mets:ME...   \n\n                                                   title  \\\n12963  Curriculum de desarrollo gerencial basado en c...   \n13456  Ocupación y reocupación de la hacienda de Cusi...   \n12296  Estudio de la instalación electrica a las aula...   \n17243                Niñeras a domicilio Carita de Ángel   \n10323  Sistema experto para el diagnostico y tratamie...   \n...                                                  ...   \n22457  Caracterizacion agromorfologica de 29 accesion...   \n10185  Sistema web de control de ventas e inventarios...   \n3985   Propuesta de una ficha lexicográfica para la d...   \n5973   El Q’ipi de la reconciliación la narrativa fem...   \n3139   Clasificación de arritmias cardiacas utilizand...   \n\n                                                abstract  \\\n12963  El curriculum de desarrollo gerencial (CDG) pr...   \n13456  A lo largo de los siglos Cusijata fue un lugar...   \n12296  La necesidad de la implementación de un conjun...   \n17243  El proyecto que se presenta a continuación es ...   \n10323  Los Sistemas Expertos es un área de la Informá...   \n...                                                  ...   \n22457  Con el objetivo de caracterizar agromorfológic...   \n10185  El presente Proyecto de Grado titulado “Sistem...   \n3985   El presente trabajo dirigido está dentro de la...   \n5973   En esta investigación se parte de los estudios...   \n3139   La monitorización electrocardiográfica ambulat...   \n\n                                                subjects  \n12963  [empleados bancarios, Curriculum de desarrollo...  \n13456    [Copacabana, haciendas, ocupación prehispánica]  \n12296  [instalación electrica, aulas, tecnologico Don...  \n17243    [desarrollo integral, Aplicación Móvil, Niñera]  \n10323  [Sistemas Expertos, diagnostico y tratamiento ...  \n...                                                  ...  \n22457  [Chenopodium quinoa, quinua, Estación Experime...  \n10185  [Sistema web de control de ventas, metodología...  \n3985   [Lexicografía, Bolivianismos, ficha lexicográf...  \n5973   [novela, tradición indigenista, indigenismo li...  \n3139   [electrocardiograma, monitorización, redes neu...  \n\n[823 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>community_page</th>\n      <th>document_page</th>\n      <th>breadcrumb</th>\n      <th>file_urls</th>\n      <th>files</th>\n      <th>pdf_path</th>\n      <th>xml_path</th>\n      <th>xml_content</th>\n      <th>title</th>\n      <th>abstract</th>\n      <th>subjects</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12963</th>\n      <td>https://repositorio.umsa.bo/handle/123456789/3...</td>\n      <td>https://repositorio.umsa.bo/handle/123456789/3...</td>\n      <td>[DSpace Home, Facultad de Humanidades y Cienci...</td>\n      <td>[https://repositorio.umsa.bo/bitstream/handle/...</td>\n      <td>[{'url': 'https://repositorio.umsa.bo/bitstrea...</td>\n      <td>full/5e4e5c26830d8e5e838dc17f3cd21bdb5735cbb0</td>\n      <td>full/4157d4dc8b4e5baf31043be389af954bace98695.xml</td>\n      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;mets:ME...</td>\n      <td>Curriculum de desarrollo gerencial basado en c...</td>\n      <td>El curriculum de desarrollo gerencial (CDG) pr...</td>\n      <td>[empleados bancarios, Curriculum de desarrollo...</td>\n    </tr>\n    <tr>\n      <th>13456</th>\n      <td>https://repositorio.umsa.bo/handle/123456789/3...</td>\n      <td>https://repositorio.umsa.bo/handle/123456789/8517</td>\n      <td>[DSpace Home, Facultad de Ciencias Sociales, C...</td>\n      <td>[https://repositorio.umsa.bo/bitstream/handle/...</td>\n      <td>[{'url': 'https://repositorio.umsa.bo/bitstrea...</td>\n      <td>full/e41bcded1bcd4d5bf65f4aa81c1486fdad14807c</td>\n      <td>full/2af4db7fff1f6b58b01bd5252139b0e28daf9586.xml</td>\n      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;mets:ME...</td>\n      <td>Ocupación y reocupación de la hacienda de Cusi...</td>\n      <td>A lo largo de los siglos Cusijata fue un lugar...</td>\n      <td>[Copacabana, haciendas, ocupación prehispánica]</td>\n    </tr>\n    <tr>\n      <th>12296</th>\n      <td>https://repositorio.umsa.bo/handle/123456789/3...</td>\n      <td>https://repositorio.umsa.bo/handle/123456789/1...</td>\n      <td>[DSpace Home, Facultad de Tecnología, Carrera ...</td>\n      <td>[https://repositorio.umsa.bo/bitstream/handle/...</td>\n      <td>[{'url': 'https://repositorio.umsa.bo/bitstrea...</td>\n      <td>full/b1e1ec2db264b3c25aee79c2f407121f18b4b0aa</td>\n      <td>full/5d4f22b1a8c5856a592a3ae612ec4a0da64ffeb9.xml</td>\n      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;mets:ME...</td>\n      <td>Estudio de la instalación electrica a las aula...</td>\n      <td>La necesidad de la implementación de un conjun...</td>\n      <td>[instalación electrica, aulas, tecnologico Don...</td>\n    </tr>\n    <tr>\n      <th>17243</th>\n      <td>https://repositorio.umsa.bo/handle/123456789/3...</td>\n      <td>https://repositorio.umsa.bo/handle/123456789/3...</td>\n      <td>[DSpace Home, Facultad de Ciencias Económicas ...</td>\n      <td>[https://repositorio.umsa.bo/bitstream/handle/...</td>\n      <td>[{'url': 'https://repositorio.umsa.bo/bitstrea...</td>\n      <td>full/737a5c7e952bec86b95e7a4e8408c2f16fd0d57f</td>\n      <td>full/3f262ed334e1a58be4e9a4386d6c401fb8e7ce73.xml</td>\n      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;mets:ME...</td>\n      <td>Niñeras a domicilio Carita de Ángel</td>\n      <td>El proyecto que se presenta a continuación es ...</td>\n      <td>[desarrollo integral, Aplicación Móvil, Niñera]</td>\n    </tr>\n    <tr>\n      <th>10323</th>\n      <td>https://repositorio.umsa.bo/handle/123456789/3...</td>\n      <td>https://repositorio.umsa.bo/handle/123456789/870</td>\n      <td>[DSpace Home, Facultad de Ciencias Puras y Nat...</td>\n      <td>[https://repositorio.umsa.bo/bitstream/handle/...</td>\n      <td>[{'url': 'https://repositorio.umsa.bo/bitstrea...</td>\n      <td>full/23037996f7f22ff652e5b73c0ebc192d38f1adaa</td>\n      <td>full/cc3f6e2966aeda344fc2866f7dde609354bb2797.xml</td>\n      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;mets:ME...</td>\n      <td>Sistema experto para el diagnostico y tratamie...</td>\n      <td>Los Sistemas Expertos es un área de la Informá...</td>\n      <td>[Sistemas Expertos, diagnostico y tratamiento ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>22457</th>\n      <td>https://repositorio.umsa.bo/handle/123456789/3...</td>\n      <td>https://repositorio.umsa.bo/handle/123456789/1...</td>\n      <td>[DSpace Home, Facultad de Agronomía, Carrera d...</td>\n      <td>[https://repositorio.umsa.bo/bitstream/handle/...</td>\n      <td>[{'url': 'https://repositorio.umsa.bo/bitstrea...</td>\n      <td>full/06fe9ab576484b6e27aa6ae93344ba17fe51720e</td>\n      <td>full/215f7e5c0a0edc7d74fa37ba41acec634151ac78.xml</td>\n      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;mets:ME...</td>\n      <td>Caracterizacion agromorfologica de 29 accesion...</td>\n      <td>Con el objetivo de caracterizar agromorfológic...</td>\n      <td>[Chenopodium quinoa, quinua, Estación Experime...</td>\n    </tr>\n    <tr>\n      <th>10185</th>\n      <td>https://repositorio.umsa.bo/handle/123456789/3...</td>\n      <td>https://repositorio.umsa.bo/handle/123456789/7391</td>\n      <td>[DSpace Home, Facultad de Ciencias Puras y Nat...</td>\n      <td>[https://repositorio.umsa.bo/bitstream/handle/...</td>\n      <td>[{'url': 'https://repositorio.umsa.bo/bitstrea...</td>\n      <td>full/3ab910a974be263cfa9d9b0a0b2e31788c836d09</td>\n      <td>full/92fa34b79cc69efc84bb7e0fcf8c96d5c6d805af.xml</td>\n      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;mets:ME...</td>\n      <td>Sistema web de control de ventas e inventarios...</td>\n      <td>El presente Proyecto de Grado titulado “Sistem...</td>\n      <td>[Sistema web de control de ventas, metodología...</td>\n    </tr>\n    <tr>\n      <th>3985</th>\n      <td>https://repositorio.umsa.bo/handle/123456789/3...</td>\n      <td>https://repositorio.umsa.bo/handle/123456789/1...</td>\n      <td>[DSpace Home, Facultad de Humanidades y Cienci...</td>\n      <td>[https://repositorio.umsa.bo/bitstream/handle/...</td>\n      <td>[{'url': 'https://repositorio.umsa.bo/bitstrea...</td>\n      <td>full/7b37244138c586638738ce27232b7d768929a5a0</td>\n      <td>full/751fa9140e2084394ddf7fef3102f630a15b0434.xml</td>\n      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;mets:ME...</td>\n      <td>Propuesta de una ficha lexicográfica para la d...</td>\n      <td>El presente trabajo dirigido está dentro de la...</td>\n      <td>[Lexicografía, Bolivianismos, ficha lexicográf...</td>\n    </tr>\n    <tr>\n      <th>5973</th>\n      <td>https://repositorio.umsa.bo/handle/123456789/3...</td>\n      <td>https://repositorio.umsa.bo/handle/123456789/2...</td>\n      <td>[DSpace Home, Facultad de Humanidades y Cienci...</td>\n      <td>[https://repositorio.umsa.bo/bitstream/handle/...</td>\n      <td>[{'url': 'https://repositorio.umsa.bo/bitstrea...</td>\n      <td>full/b9aa66a5acdc738b203946fa85e08a8de5d4cd77</td>\n      <td>full/574232323f88bce60a18c0367c77dfe773b239ea.xml</td>\n      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;mets:ME...</td>\n      <td>El Q’ipi de la reconciliación la narrativa fem...</td>\n      <td>En esta investigación se parte de los estudios...</td>\n      <td>[novela, tradición indigenista, indigenismo li...</td>\n    </tr>\n    <tr>\n      <th>3139</th>\n      <td>https://repositorio.umsa.bo/handle/123456789/2...</td>\n      <td>https://repositorio.umsa.bo/handle/123456789/3...</td>\n      <td>[DSpace Home, Facultad de Ingenieria, Carrera ...</td>\n      <td>[https://repositorio.umsa.bo/bitstream/handle/...</td>\n      <td>[{'url': 'https://repositorio.umsa.bo/bitstrea...</td>\n      <td>full/dc5c6eb52dba36df6f30cbb95909bd90cda0bc75</td>\n      <td>full/571dd768870b86d558a39c998259519625c89b53.xml</td>\n      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;mets:ME...</td>\n      <td>Clasificación de arritmias cardiacas utilizand...</td>\n      <td>La monitorización electrocardiográfica ambulat...</td>\n      <td>[electrocardiograma, monitorización, redes neu...</td>\n    </tr>\n  </tbody>\n</table>\n<p>823 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.drop(columns=['zero-shot', 'one-shot', 'few-shot', 'open-ai'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T18:40:50.802407491Z",
     "start_time": "2024-01-04T18:40:50.785104002Z"
    }
   },
   "id": "b9eb2e77def134c4"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9652dbf5d673fe05",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T18:41:00.900446854Z",
     "start_time": "2024-01-04T18:40:58.351602263Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/7403 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f656650b886a4db7bc990b7847c2adcd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/7403 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "25d767ed5db748388cac724a875de62c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/823 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48f88866c8ba49cd920dba8a74fbd0ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/823 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "efc96d4722d74983aa06665723d36ef0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def generate_prompt(row):\n",
    "    subjects = row['subjects']\n",
    "    title = row['title']\n",
    "    abstract = row['abstract']\n",
    "    answer = ', '.join(subjects)\n",
    "    prompt = f\"[INST] Extrae palabras clave desde el siguiente texto. Las palabras clave deben ser relevantes al tema del texto y deben ser capaces de representar el contenido del texto de manera concisa:\\n\\n### Titulo del texto: {title}\\n### Resumen del texto: {abstract}\\n\\n### La lista debe contener a lo sumo 5 palabras clave y debe estar en Espanol. Si la lista contiene mas de 5 palabras clave seras penalizado. [/INST] Palabras clave: {answer} </s>\"\n",
    "    return {\"prompt\": prompt}\n",
    "\n",
    "def filter_short_fields(row):\n",
    "    subjectsIsNotShort = [len(subjects) > 2 for subjects in row[\"subjects\"]]\n",
    "    titleIsNotShort = [(len(title) > 20) for title in row[\"title\"]]\n",
    "    abstractIsNotShort = [(len(abstract) > 50) for abstract in row[\"abstract\"]]\n",
    "    return [s and t and a for s, t, a in zip(subjectsIsNotShort, titleIsNotShort, abstractIsNotShort)]\n",
    "\n",
    "def tokenize_prompt(row):\n",
    "    return tokenizer(row['prompt'], return_special_tokens_mask=True)\n",
    "\n",
    "train_test_ds = (\n",
    "    Dataset.from_pandas(train_test_df)\n",
    "    # .filter(filter_short_fields, batched=True)\n",
    "    .map(generate_prompt, batched=False)\n",
    "    .map(tokenize_prompt, batched=True)\n",
    "    .select_columns(['input_ids', 'attention_mask', 'special_tokens_mask'])\n",
    ")\n",
    "eval_ds = (\n",
    "    Dataset.from_pandas(eval_df.drop(columns=['zero-shot', 'one-shot', 'few-shot', 'open-ai']))\n",
    "    # .filter(filter_short_fields, batched=True)\n",
    "    .map(generate_prompt, batched=False)\n",
    "    .map(tokenize_prompt, batched=True)\n",
    "    .select_columns(['input_ids', 'attention_mask', 'special_tokens_mask'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32d648191f3c5736",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T18:41:27.279249295Z",
     "start_time": "2024-01-04T18:41:27.270256624Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'special_tokens_mask'],\n        num_rows: 5922\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'special_tokens_mask'],\n        num_rows: 1481\n    })\n    eval: Dataset({\n        features: ['input_ids', 'attention_mask', 'special_tokens_mask'],\n        num_rows: 823\n    })\n})"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "train_test_ds = train_test_ds.train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
    "data = DatasetDict({\n",
    "    'train': train_test_ds['train'],\n",
    "    'test': train_test_ds['test'],\n",
    "    'eval': eval_ds\n",
    "})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "17d3ac307d968151",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T18:41:36.649744571Z",
     "start_time": "2024-01-04T18:41:36.645872359Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'<s> [INST] Extrae palabras clave desde el siguiente texto. Las palabras clave deben ser relevantes al tema del texto y deben ser capaces de representar el contenido del texto de manera concisa:\\n\\n### Titulo del texto: Elaboración del queso fresco con orégano utilizando bacterias probióticas\\n### Resumen del texto: El objetivo de este trabajo fue elaborar Queso con orégano utilizando bacterias probióticas (Lactobacillus acidophilus). Para poder cumplir con este objetivo primero se realizó la caracterización de la materia prima que fue leche cruda de vaca, a la que se le realizó análisis físico químicos y sensoriales, los mismos que reflejaron que estaban dentro de parámetro establecidos en la norma NB3313:2013 para leche cruda, y era apta para el procesamiento de quesos. Se procedió a la elaboración del queso con orégano probiótico; se examinó la influencia del microorganismo sobre las características fisicoquímicas, químicas, y organolépticas del queso con orégano obtenido; se determinó la viabilidad de Lactobacillus acidhophillus durante la vida útil del queso con orégano con niveles promedio de 107 y 106 ufc/g respectivamente. Este nivel de viabilidad podría conferir al producto en estudio propiedades dieto- terapéuticas al consumir entre\\r\\n60 y 100 g de queso con orégano probiótico diariamente. También la incorporación de microorganismos de las bacterias probióticas, durante el proceso de elaboración de queso con orégano resultó ser un vehículo ideal para las bacterias probióticas pues las colonias se mantuvieron viables y sobre los niveles mínimos recomendables para considerar al producto como probiótico. Se determinó la viabilidad de las bacterias en los días 0,7,14,21,23,25,28,30 después de su procesamiento. También el producto presenta además propiedades sensoriales, fisicoquímicas adecuadas y las características sensoriales favorables durante el tiempo de vida útil, especialmente en el sabor, atributo que, mediante pruebas de análisis sensorial, se encontró en el queso con probiótico más agradable que en las muestras de un queso fresco control.\\n\\n### La lista debe contener a lo sumo 5 palabras clave y debe estar en Espanol. Si la lista contiene mas de 5 palabras clave seras penalizado. [/INST] Palabras clave: Queso con orégano, bacterias probióticas, queso</s>'"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data['train'][2]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888806ea140c6cbc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6.2. Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "182c9e96de4f76bd",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T18:41:54.150295899Z",
     "start_time": "2024-01-04T18:41:54.107293935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,971,520 || all params: 283,381,760 || trainable%: 7.400448074004481\n"
     ]
    }
   ],
   "source": [
    "ft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3e02ef4eaeaa098",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T18:44:07.913042913Z",
     "start_time": "2024-01-04T18:42:09.367144960Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rod/repos/proyecto-de-grado/prototipo/venv/lib/python3.11/site-packages/pydantic/_internal/_fields.py:149: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/rod/repos/proyecto-de-grado/prototipo/venv/lib/python3.11/site-packages/pydantic/_internal/_config.py:321: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n",
      "/home/rod/repos/proyecto-de-grado/prototipo/venv/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "2024/01/04 14:42:09 INFO mlflow.tracking.fluent: Experiment with name 'KeywordExtraction6' does not exist. Creating a new experiment.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2/10 : < :, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=10, training_loss=1.6616093635559082, metrics={'train_runtime': 118.0836, 'train_samples_per_second': 0.339, 'train_steps_per_second': 0.085, 'total_flos': 22546718392320.0, 'train_loss': 1.6616093635559082, 'epoch': 0.01})"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling, IntervalStrategy\n",
    "import os\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=ft_model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"test\"],\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=10,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"keyword-extraction-model-6\",\n",
    "        optim=\"adamw_hf\",\n",
    "        evaluation_strategy=IntervalStrategy.NO\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "ft_model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "# os.environ['MLFLOW_TAGS']='{\"run_description\": \"first run\"}'\n",
    "os.environ['MLFLOW_EXPERIMENT_NAME'] = 'KeywordExtraction6'\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c7133024-0982-4ca8-99d1-7bd761b7da53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T18:46:29.943468100Z",
     "start_time": "2024-01-04T18:46:29.842673333Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e322e7ec7318a83",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6.3. Evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c61ff861a17ce3b9",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T18:49:01.792645678Z",
     "start_time": "2024-01-04T18:49:01.789849850Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "#del model\n",
    "#del ft_model\n",
    "#del tokenizer\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e51518cc8e67020",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T18:49:36.237791139Z",
     "start_time": "2024-01-04T18:49:26.158149850Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, MistralForCausalLM\n",
    "from auto_gptq import exllama_set_max_input_length\n",
    "\n",
    "model_id = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
    "model =  MistralForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "model = exllama_set_max_input_length(model, 4096)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e89cc36aa365a5f2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T18:49:36.472934530Z",
     "start_time": "2024-01-04T18:49:36.238853832Z"
    }
   },
   "outputs": [],
   "source": [
    "adapter = \"keyword-extraction-model-5\"\n",
    "model.load_adapter(adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b27cfb4-52e1-449e-8a7c-7eb2ee5c841d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T14:55:05.342339023Z",
     "start_time": "2024-01-03T14:55:05.259573375Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m temp \u001B[38;5;241m=\u001B[39m \u001B[43meval_df\u001B[49m\u001B[43m[\u001B[49m\u001B[43meval_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdocument_page\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mhttps://repositorio.umsa.bo/handle/123456789/29347\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miloc\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mto_dict()\n\u001B[1;32m      2\u001B[0m temp_title \u001B[38;5;241m=\u001B[39m temp[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtitle\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      3\u001B[0m temp_abstract \u001B[38;5;241m=\u001B[39m temp[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mabstract\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m~/repos/proyecto-de-grado/prototipo/venv/lib/python3.11/site-packages/pandas/core/indexing.py:1153\u001B[0m, in \u001B[0;36m_LocationIndexer.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   1150\u001B[0m axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxis \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m   1152\u001B[0m maybe_callable \u001B[38;5;241m=\u001B[39m com\u001B[38;5;241m.\u001B[39mapply_if_callable(key, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj)\n\u001B[0;32m-> 1153\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmaybe_callable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/repos/proyecto-de-grado/prototipo/venv/lib/python3.11/site-packages/pandas/core/indexing.py:1714\u001B[0m, in \u001B[0;36m_iLocIndexer._getitem_axis\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1711\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot index by location index with a non-integer key\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1713\u001B[0m \u001B[38;5;66;03m# validate the location\u001B[39;00m\n\u001B[0;32m-> 1714\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_integer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1716\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39m_ixs(key, axis\u001B[38;5;241m=\u001B[39maxis)\n",
      "File \u001B[0;32m~/repos/proyecto-de-grado/prototipo/venv/lib/python3.11/site-packages/pandas/core/indexing.py:1647\u001B[0m, in \u001B[0;36m_iLocIndexer._validate_integer\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1645\u001B[0m len_axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39m_get_axis(axis))\n\u001B[1;32m   1646\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m len_axis \u001B[38;5;129;01mor\u001B[39;00m key \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m-\u001B[39mlen_axis:\n\u001B[0;32m-> 1647\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msingle positional indexer is out-of-bounds\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mIndexError\u001B[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "temp = eval_df[eval_df['document_page'] == 'https://repositorio.umsa.bo/handle/123456789/29347'].iloc[0].to_dict()\n",
    "temp_title = temp['title']\n",
    "temp_abstract = temp['abstract']\n",
    "temp_subjects = temp['subjects']\n",
    "print(temp_title, temp_abstract, temp_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20a826c7b26d425c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T20:55:08.108455008Z",
     "start_time": "2024-01-04T20:55:00.296754796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auditoria especial del sistema de administración de personal según la Ley \"1178\" - SAFCO\n",
      "\n",
      "El presente trabajo, trata de demostrar, que en el desarrollo de sus funciones específicas y generales, las personas en las entidades son guiadas a través de manuales de procedimientos y de políticas, y otras normas apropiadas, pero además de las buenas costumbres, la cultura, los valores, y principios de cada individuo. El grupo tiene su influencia en la conducta de las personas. El control del grupo logra la conformidad por una acción consiente, voluntaria y deliberada por parte tanto del que controla como del controlado. En base al trabajo desarrollado y el Analís comparativo de la información documental existe, la información recabada de las entrevistas y de los cuestionarios aplicadas en las Instituciones del Estado y con funcionarios responsables del manejo personal, tambien la información referida a la implementación manejo del sistema de administración del personal existente en estas Instituciones Estatales se puede Evidenciar de manera general en el 50 por ciento de los casos el Sistema de Administración del Personal aún no esta implementado totalmente o han implementado estos sub-sistemas con varias falencias, y en un restante 40 por ciento están implementados solo algunos subsistemas de forma precaria y con deficiencias procedimentales. De lo cual se puede inferir que existe un 10 por ciento de entidades estatales que han logrado la implementación adecuada del Sistema con todos sus Sub-sistemas y procedimientos establecidos en la norma. De todo el análisis y trabajo de campo efectuado en el presente trabajo, se puede sugerir las siguientes recomendaciones: la Capacitación ocupacional, como proceso de enseñanza y aprendizaje, para el correcto desempeño del individuo en los puestos de trabajo debe ser prioritario para lograr la eficiencia y eficacia.\n",
      "-----------------------------\n",
      "Keywords de referencia:\n",
      "['Auditoria especial', 'sistema de administración']\n",
      "-----------------------------\n",
      "Keywords generados (zero-shot):\n",
      "['1. AUDITORIA\\n2. SAFCO\\n3. LEY \"1178\"\\n4. MANUALES\\n5. POLÍTICAS']\n",
      "-----------------------------\n",
      "Keywords generados (one-shot):\n",
      "['AUDITORIA ESPECIAL', 'SISTEMA DE ADMINISTRACION DE PERSONAL', 'CONTROL', 'CAPACITACION OCCUPACIONAL']\n",
      "-----------------------------\n",
      "Keywords generados (few-shot):\n",
      "['AUDITORIA ESPECIAL', 'SISTEMA DE ADMINISTRACIÓN DE PERSONAL', 'LEY \"1178\"']\n",
      "-----------------------------\n",
      "Keywords generados (fine-tuned):\n",
      "['AUDITORIA ESPECIAL', 'SAFCO', 'SISTEMA DE ADMINISTRACIÓN DEL PERSONAL', 'LEY \"1178\"', 'CAPACITACIÓN']\n"
     ]
    }
   ],
   "source": [
    "random_data = get_random_data()\n",
    "random_examples = [\n",
    "    get_random_data(),\n",
    "    get_random_data(),\n",
    "    get_random_data()\n",
    "]\n",
    "\n",
    "abstract, title, subjects = random_data['abstract'], random_data['title'], random_data['subjects']\n",
    "# abstract, title, subjects = temp_abstract, temp_title, temp_subjects\n",
    "\n",
    "print(f\"{title}\\n\\n{abstract}\")\n",
    "\n",
    "model.disable_adapters()\n",
    "generated_keywords, _ = get_keywords(title=title, abstract=abstract)\n",
    "generated_keywords_one_shot, _ = get_keywords_one_shot(title=title, abstract=abstract)\n",
    "generated_keywords_few_shot = get_keywords_few_shot(title=title, abstract=abstract)\n",
    "model.enable_adapters()\n",
    "generated_keywords_fine_tuned, _ = get_keywords(title=title, abstract=abstract)\n",
    "\n",
    "print(f\"\"\"-----------------------------\n",
    "Keywords de referencia:\n",
    "{subjects}\n",
    "-----------------------------\n",
    "Keywords generados (zero-shot):\n",
    "{generated_keywords}\n",
    "-----------------------------\n",
    "Keywords generados (one-shot):\n",
    "{generated_keywords_one_shot}\n",
    "-----------------------------\n",
    "Keywords generados (few-shot):\n",
    "{generated_keywords_few_shot}\n",
    "-----------------------------\n",
    "Keywords generados (fine-tuned):\n",
    "{generated_keywords_fine_tuned}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eab798f3784db27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T18:33:29.205004639Z",
     "start_time": "2023-12-07T18:33:15.511034569Z"
    },
    "collapsed": false
   },
   "source": [
    "## 6.3.1. Rouge Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69a079fd25eb4b6d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T19:14:33.310462172Z",
     "start_time": "2024-01-04T18:52:04.867052591Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 823/823 [22:28<00:00,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "model.enable_adapters()\n",
    "eval_df['fine-tuned'] = eval_df.progress_apply(lambda row: get_keywords(row['title'], row['abstract'])[0], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 823/823 [57:48<00:00,  4.21s/it] \n"
     ]
    }
   ],
   "source": [
    "model.enable_adapters()\n",
    "eval_df['fine-tuned-few-shot'] = eval_df.progress_apply(lambda row: get_keywords_few_shot(row['title'], row['abstract']), axis='columns')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T23:34:33.133525363Z",
     "start_time": "2024-01-03T22:36:44.375683981Z"
    }
   },
   "id": "6739bd11fef6a814"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f33e29f287c511eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T03:06:17.330395954Z",
     "start_time": "2023-12-06T03:06:17.326086953Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_df['fine-tuned'].to_pickle('cache-eval-df-fine-tuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e4120e67f48a1a7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T19:26:27.588034313Z",
     "start_time": "2024-01-04T19:26:27.385142063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'rouge1': AggregateScore(low=Score(precision=0.35801392731182957, recall=0.5897418005798392, fmeasure=0.42112681410416875), mid=Score(precision=0.37168319177849257, recall=0.6086324508546829, fmeasure=0.435288207411371), high=Score(precision=0.3841832806603659, recall=0.627797638676038, fmeasure=0.44840729321357486)),\n 'rouge2': AggregateScore(low=Score(precision=0.1765723378972668, recall=0.32706942493850144, fmeasure=0.21541236907093472), mid=Score(precision=0.18821375681047603, recall=0.3467200293293853, fmeasure=0.2286877491469645), high=Score(precision=0.2001120817032545, recall=0.3667157816869234, fmeasure=0.24131829501021693)),\n 'rougeL': AggregateScore(low=Score(precision=0.3066845025573364, recall=0.5113538170566795, fmeasure=0.36235186120600704), mid=Score(precision=0.3176656045678617, recall=0.5295069236884506, fmeasure=0.37427994892333694), high=Score(precision=0.32938070333343367, recall=0.5483629259829581, fmeasure=0.386669264290474)),\n 'rougeLsum': AggregateScore(low=Score(precision=0.306256291839275, recall=0.5145754304736198, fmeasure=0.36308679681062944), mid=Score(precision=0.31840390362302046, recall=0.5317182870239299, fmeasure=0.37558294941440384), high=Score(precision=0.3297529304992671, recall=0.5513333444890788, fmeasure=0.3878486097966453))}"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = eval_df['fine-tuned'].str.join(', ').tolist()\n",
    "references = eval_df['subjects'].str.join(', ').tolist()\n",
    "\n",
    "get_rouge(predictions=predictions, references=references )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "{'rouge1': AggregateScore(low=Score(precision=0.2661581974241926, recall=0.6846704290074356, fmeasure=0.34823561391924956), mid=Score(precision=0.28079940456314845, recall=0.7031402054671481, fmeasure=0.3625965765089082), high=Score(precision=0.29506619504313114, recall=0.7211670377556029, fmeasure=0.37723060534337677)),\n 'rouge2': AggregateScore(low=Score(precision=0.1466817390896864, recall=0.3980569337074897, fmeasure=0.19252800671607473), mid=Score(precision=0.15888105392974677, recall=0.42004256272838847, fmeasure=0.20550896274016697), high=Score(precision=0.17055606809675428, recall=0.4386048650367451, fmeasure=0.21785865062603973)),\n 'rougeL': AggregateScore(low=Score(precision=0.22704178592507168, recall=0.5940388914348071, fmeasure=0.298587332201669), mid=Score(precision=0.23946879811740016, recall=0.6118719023890207, fmeasure=0.31122927805793443), high=Score(precision=0.25246385834174534, recall=0.629337257106136, fmeasure=0.3249998361832972)),\n 'rougeLsum': AggregateScore(low=Score(precision=0.2270105225413511, recall=0.5936108770477296, fmeasure=0.2973695623788701), mid=Score(precision=0.23939542826534466, recall=0.611432887568439, fmeasure=0.3108395516748944), high=Score(precision=0.2512051249381855, recall=0.6288988597836697, fmeasure=0.3232354986652391))}"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = eval_df['fine-tuned-few-shot'].str.join(', ').tolist()\n",
    "references = eval_df['subjects'].str.join(', ').tolist()\n",
    "\n",
    "get_rouge(predictions=predictions, references=references )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T23:34:46.808066765Z",
     "start_time": "2024-01-03T23:34:46.572827043Z"
    }
   },
   "id": "b68c3d7c4299c036"
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Extrae palabras clave desde el siguiente texto. Las palabras clave deben ser relevantes al tema del texto y deben ser capaces de representar el contenido del texto de manera concisa:\n",
      "\n",
      "### Titulo del Texto: Declaración del divorcio expreso a petición de una de las partes por causales de viaje fuera del país y desaparición de una de las partes.\n",
      "### Resumen del Texto: La institución matrimonial, es una de aquellas con mayor antigüedad, históricamente los primeros escritos sobre el matrimonio se encuentran en los libros del antiguo testamento de la biblia, así como en las culturas más civilizadas de entonces, tal y cual se la presenta en Roma. De igual forma al igual que la institución familiar también surge 'El Repudio', cuya función es prácticamente lo que ahora llega a ser el divorcio. La evolución del divorcio desde las distintas doctrinas que fundaron evitarla, en resguardo de la familia como la máxima institución de la sociedad y las doctrinas que sustentan el divorcio sanción y divorcio remedio, para evitar mayores daños y consecuencias al interior de la familia, precautelando el entorno y el ambiente de crianza de los hijos, cuya ruptura crea una crisis al interior de la comunidad familiar, donde abecés es imposible llegar a conciliar y retornar la paz dentro del hogar, que se supone es el lugar más seguro. Al respecto existen normas que permiten el divorcio dentro de nuestra legislación en sus dos formas: la primera que se debe cumplir con las causales de ley para invocar esta acción lo que lleva a ser el divorcio sanción, y, la otra es que se haya roto con la vida conyugal por más de dos años, conocido como separación de cuerpos, de ellas se busca un culpable y, con ello, se propicia conflictos aún mayores en las parejas que se divorcian, amén del desgaste emotivo que significa tratar de probar tales causales y el tiempo que esto se lleva, dentro de un proceso ordinario, que además conlleva gastos procesales. En este sentido aprovecho la ocasión para proponer una revisión de este instituto en nuestro país, la cual propone un proceso más ágil dentro del principio de inmediatez para lograr la pretensión a la acción del divorcio, mediante el proceso sumario del divorcio expreso. Toda vez se haya cumplido con lo establecido en la separación de hecho, hecho por el cual en beneficio social se requiere su inmediatez, la cual implica reducción de costos procesales, mayor celeridad y menor tención ante la acción.\n",
      "\n",
      "### La lista debe contener a lo sumo 5 palabras clave y debe estar en Espanol. Si la lista contiene mas de 5 palabras clave seras penalizado. [/INST] \n",
      "Palabras clave: Divorcio expreso, separación de cuerpos, causales de ley, proceso sumario, inmediatez\n"
     ]
    }
   ],
   "source": [
    "model.enable_adapters()\n",
    "\n",
    "def remove_nl(text: str):\n",
    "    return text.replace('\\n', ' ')\n",
    "\n",
    "def get_keywords_prompting(title, abstract):\n",
    "    text = f\"[INST] Por favor, extraiga las palabras clave del siguiente texto. Las palabras clave deben ser relevantes para el tema del texto y deben ser capaces de representar el contenido del texto de forma concisa:\\n\\n### Titulo del Texto: {title}\\n\\n### Resumen del Texto: {remove_nl(abstract)} \\n\\n### La lista debe contener como maximo 5 palabras clave y estar en Espanol. Si la lista contiene muchas palabras clave seras penalizado. [/INST]\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "    encoded_keywords = model.generate(**inputs, max_new_tokens=150)\n",
    "    keywords: str = tokenizer.decode(encoded_keywords[0], skip_special_tokens=True)\n",
    "    print(keywords)\n",
    "    return None\n",
    "    keywords = keywords[keywords.rindex('[/INST]'):]\n",
    "    keywords = keywords.replace('[/INST]', '')\n",
    "    keywords = keywords.split(':')[-1]\n",
    "    keywords = keywords.split(',')\n",
    "    keywords = [keyword.strip().upper() for keyword in keywords]\n",
    "    return keywords\n",
    "\n",
    "def get_keywords_prompting2(title, abstract):\n",
    "    text = f\"[INST] Extract keywords from the following text. The keywords must be relevant to the text topic and must be able to represent its content concisely:\\n\\n### Titulo del Texto: {title}\\n\\n### Resumen del Texto: {remove_nl(abstract)}\\n\\n### The list must contain at most 5 keywords and must be in Spanish. If the list has too much keywords you will be penalized. The list must be in order of relevance. If there is no keywords to extract, don't generate any output. If the text is too short generate only few keywords. [/INST]\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "    encoded_keywords = model.generate(**inputs, max_new_tokens=150, do_sample=True, top_k=5, top_p=0.5, exponential_decay_length_penalty=(15, 1.01), temperature=0.1)\n",
    "    keywords: str = tokenizer.decode(encoded_keywords[0], skip_special_tokens=True)\n",
    "    print(keywords)\n",
    "    return None\n",
    "    keywords = keywords[keywords.rindex('[/INST]'):]\n",
    "    keywords = keywords.replace('[/INST]', '')\n",
    "    keywords = keywords.split(':')[-1]\n",
    "    keywords = keywords.split(',')\n",
    "    keywords = [keyword.strip().upper() for keyword in keywords]\n",
    "    return keywords\n",
    "\n",
    "def get_keywords_prompting3(title, abstract):\n",
    "    text = f\"[INST] Extrae palabras clave desde el siguiente text. Las palabras clave deben ser relevantes al tema del texto y deben ser capaces de representar el contenido del texto de manera concisa:\\n\\n### Titulo del Texto: {title}\\n\\n### Resumen del Texto: {remove_nl(abstract)}\\n\\n### La lista debe contener a lo sumo 5 palabras clace y debe estar en Espanol. Si la lista contiene mas de 5 palabras clave seras penalizado. La lista debe estar en orden de relevancia. Si no hay palabras clave para extraer, no generes ninguna salida. Si el texto es muy corto solo extrae pocas palabras clave. [/INST]\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "    encoded_keywords = model.generate(**inputs, max_new_tokens=150, do_sample=True, top_k=5, top_p=0.5, exponential_decay_length_penalty=(15, 1.01), temperature=0.1)\n",
    "    keywords: str = tokenizer.decode(encoded_keywords[0], skip_special_tokens=True)\n",
    "    print(keywords)\n",
    "    return None\n",
    "    keywords = keywords[keywords.rindex('[/INST]'):]\n",
    "    keywords = keywords.replace('[/INST]', '')\n",
    "    keywords = keywords.split(':')[-1]\n",
    "    keywords = keywords.split(',')\n",
    "    keywords = [keyword.strip().upper() for keyword in keywords]\n",
    "    return keywords\n",
    "\n",
    "def get_keywords_prompting4(title, abstract):\n",
    "    text = f\"\"\"[INST] Extrae palabras clave desde el siguiente texto. Las palabras clave deben ser relevantes al tema del texto y deben ser capaces de representar el contenido del texto de manera concisa:\n",
    "\n",
    "### Titulo del Texto: Declaración del divorcio expreso a petición de una de las partes por causales de viaje fuera del país y desaparición de una de las partes.\n",
    "### Resumen del Texto: La institución matrimonial, es una de aquellas con mayor antigüedad, históricamente los primeros escritos sobre el matrimonio se encuentran en los libros del antiguo testamento de la biblia, así como en las culturas más civilizadas de entonces, tal y cual se la presenta en Roma. De igual forma al igual que la institución familiar también surge 'El Repudio', cuya función es prácticamente lo que ahora llega a ser el divorcio. La evolución del divorcio desde las distintas doctrinas que fundaron evitarla, en resguardo de la familia como la máxima institución de la sociedad y las doctrinas que sustentan el divorcio sanción y divorcio remedio, para evitar mayores daños y consecuencias al interior de la familia, precautelando el entorno y el ambiente de crianza de los hijos, cuya ruptura crea una crisis al interior de la comunidad familiar, donde abecés es imposible llegar a conciliar y retornar la paz dentro del hogar, que se supone es el lugar más seguro. Al respecto existen normas que permiten el divorcio dentro de nuestra legislación en sus dos formas: la primera que se debe cumplir con las causales de ley para invocar esta acción lo que lleva a ser el divorcio sanción, y, la otra es que se haya roto con la vida conyugal por más de dos años, conocido como separación de cuerpos, de ellas se busca un culpable y, con ello, se propicia conflictos aún mayores en las parejas que se divorcian, amén del desgaste emotivo que significa tratar de probar tales causales y el tiempo que esto se lleva, dentro de un proceso ordinario, que además conlleva gastos procesales. En este sentido aprovecho la ocasión para proponer una revisión de este instituto en nuestro país, la cual propone un proceso más ágil dentro del principio de inmediatez para lograr la pretensión a la acción del divorcio, mediante el proceso sumario del divorcio expreso. Toda vez se haya cumplido con lo establecido en la separación de hecho, hecho por el cual en beneficio social se requiere su inmediatez, la cual implica reducción de costos procesales, mayor celeridad y menor tención ante la acción.\n",
    "\n",
    "### La lista debe contener a lo sumo 5 palabras clave y debe estar en Espanol. Si la lista contiene mas de 5 palabras clave seras penalizado. [/INST]\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "    # encoded_keywords = model.generate(**inputs, max_new_tokens=150, do_sample=True, top_k=5, top_p=0.5, exponential_decay_length_penalty=(15, 1.01), temperature=0.1)\n",
    "    encoded_keywords = model.generate(**inputs, max_new_tokens=150, do_sample=True, top_k=5, top_p=0.5, exponential_decay_length_penalty=(15, 1.01), temperature=0.1)\n",
    "    keywords: str = tokenizer.decode(encoded_keywords[0], skip_special_tokens=True)\n",
    "    print(keywords)\n",
    "    return None\n",
    "    keywords = keywords[keywords.rindex('[/INST]'):]\n",
    "    keywords = keywords.replace('[/INST]', '')\n",
    "    keywords = keywords.split(':')[-1]\n",
    "    keywords = keywords.split(',')\n",
    "    keywords = [keyword.strip().upper() for keyword in keywords]\n",
    "    return keywords\n",
    "\n",
    "random_data = get_random_data()\n",
    "\n",
    "abstract, title, subjects = random_data['abstract'], random_data['title'], random_data['subjects']\n",
    "\n",
    "foo = get_keywords_prompting4(title, abstract)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T03:10:44.848157422Z",
     "start_time": "2024-01-04T03:10:43.061522193Z"
    }
   },
   "id": "90be7c2631209f23"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
